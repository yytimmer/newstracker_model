{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import random\n",
    "from Levenshtein import distance as levenshtein_distance, editops, seqratio\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk import SnowballStemmer\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "from scipy.sparse import csr_matrix\n",
    "from telwoord import ordinal, cardinal\n",
    "from datetime import datetime\n",
    "import re\n",
    "import math\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Dutch spacy model\n",
    "\n",
    "nlp = spacy.load(\"nl_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Dutch stop words\n",
    "\n",
    "stop_words = get_stop_words('nl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import list of common Dutch words\n",
    "\n",
    "dutch_dictionary = open(\"..\\\\Data\\\\data_processing_knowledge_input\\\\dutch_dictionary.txt\", encoding='utf-8', newline='')\n",
    "\n",
    "dutch_words = dutch_dictionary.readlines()\n",
    "dutch_words = [word.replace('\\n', '').lower() for word in dutch_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that splits string into array of sentences based on regex\n",
    "def split_text(text):\n",
    "    text_splitted = []\n",
    "    text_parts = re.split(\"\\n\", text)\n",
    "                    \n",
    "    for part in text_parts:\n",
    "        sentences = re.split(\"(?:(?<=[\\\\!\\\\?\\\\.])|(?<=\\\\.”)|(?<=\\\\.”)|(?<=\\\\.')|(?<=\\\\?\\\")|(?<=\\\\?”)|(?<=\\\\?')|(?<=\\\\!\\\")|(?<=\\\\!')|(?<=\\\\!”)) |\\\\u00a0|\\n\", part)\n",
    "        text_splitted.append(sentences)\n",
    "        \n",
    "    return text_splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help function to compute features related to numerical information in the atomic change\n",
    "def compare_numbers(original_number, new_number):\n",
    "    if original_number > new_number:\n",
    "        return 1\n",
    "            \n",
    "    if original_number == new_number:\n",
    "        return 2\n",
    "                \n",
    "    if original_number < new_number:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lists of doubt words, certain words, negation words and temporary/approximate words that can be used to compute several features\n",
    "\n",
    "doubt_words = ['vaak', 'meestal', 'dikwijls', 'vooral',\n",
    "               'soms', 'misschien', 'mogelijk', 'mogelijks', 'waarschijnlijk', 'wellicht', 'eventueel',\n",
    "                  'in principe', 'op zich', 'zouden', 'kunnen', 'zou', 'kan', 'lijkt', 'lijken',\n",
    "               'voorlopig', 'tijdelijk', 'even',\n",
    "              'doorgaans', 'typisch', 'af en toe', 'zelden', 'sporadisch', 'incidenteel', 'weleens', \n",
    "              'deels', 'gedeeltelijk', 'bijna', 'onzeker', 'sommige',  'quasi',\n",
    "              'een paar', 'enkele', 'ettelijke', 'weinige', 'enkele', 'een paar', 'enige',\n",
    "                 'bijvoorbeeld', 'onder meer', 'onder andere', 'bv.', 'gans', 'een deel', 'vermoedelijk']\n",
    "\n",
    "certain_words = ['altijd', 'nooit', 'volledig', 'helemaal', 'zeker', 'absoluut', 'alleszins', 'beslist', 'gegarandeerd', 'gewis', 'natuurlijk', 'ongetwijfeld', 'ontwijfelbaar', 'overtuigd', \n",
    "                'stellig', 'verzekerd', 'wis', 'zekerlijk', 'meeste', 'alle', 'geen enkele', 'niemand', 'iedereen', \n",
    "                'niets', 'alles', 'gans', 'duidelijk', 'afgetekend',\n",
    "                 'apert', 'evident', 'flagrant', 'helder', 'klaarblijkend', 'klaarblijkelijk', 'merkelijk',\n",
    "                 'onbetwistbaar', 'ondubbelzinnig', 'onloochenbaar', 'onmiskenbaar', 'onomstotelijk', 'ontegensprekelijk',\n",
    "                 'ontegenzeggelijk', 'ontegenzeglijk', 'onweerlegbaar', 'overduidelijk', 'zonneklaar']\n",
    "\n",
    "negation_words = ['geen', 'niet']\n",
    "\n",
    "temporary_words = ['minstens', 'bijna', 'zeker', 'circa', 'ongeveer', 'zo''n', 'tal', 'tallen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Dutch names for colors\n",
    "\n",
    "colors = {}\n",
    "color_file = open('..\\\\Data\\\\data_processing_knowledge_input\\\\colors.csv', newline='', encoding='utf-8')\n",
    "reader = csv.reader(color_file, delimiter=',', quotechar='\"')\n",
    "\n",
    "for line in reader:\n",
    "    colors[line[0].lower()] = line[1].lower()\n",
    "    colors[line[1].lower()] = line[0].lower()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Dutch names for days of the week\n",
    "\n",
    "days = set()\n",
    "days_file = open('..\\\\Data\\\\data_processing_knowledge_input\\\\days_of_the_week.csv', newline='', encoding='utf-8')\n",
    "reader = csv.reader(days_file, delimiter=',', quotechar='\"')\n",
    "\n",
    "for line in reader:\n",
    "    days.add(line[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Dutch names for currencies\n",
    "\n",
    "currencies = set()\n",
    "currencies_file = open('..\\\\Data\\\\data_processing_knowledge_input\\\\currencies.csv', newline='', encoding='utf-8')\n",
    "reader = csv.reader(days_file, delimiter=',', quotechar='\"')\n",
    "\n",
    "for line in reader:\n",
    "    currencies.add(line[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Dutch names for months\n",
    "\n",
    "months = set()\n",
    "months_file = open('..\\\\Data\\\\data_processing_knowledge_input\\\\months.csv', newline='', encoding='utf-8')\n",
    "reader = csv.reader(months_file, delimiter=',', quotechar='\"')\n",
    "\n",
    "for line in reader:\n",
    "    months.add(line[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Dutch names for wind directions\n",
    "\n",
    "wind_directions = set()\n",
    "wind_directions_file = open('..\\\\Data\\\\data_processing_knowledge_input\\\\wind_directions.csv', newline='', encoding='utf-8')\n",
    "reader = csv.reader(wind_directions_file, delimiter=',', quotechar='\"')\n",
    "\n",
    "for line in reader:\n",
    "    wind_directions.add(line[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Dutch names for states and provinces\n",
    "\n",
    "states_and_provinces = {}\n",
    "states_and_provinces_file = open('..\\\\Data\\\\data_processing_knowledge_input\\\\states_and_provinces.csv', newline='', encoding='utf-8')\n",
    "reader = csv.reader(states_and_provinces_file, delimiter=',', quotechar='\"')\n",
    "\n",
    "for line in reader:\n",
    "    for i in range(0, 5):\n",
    "        for j in range(0, 5):\n",
    "            if i != j:\n",
    "                if not(line[i] in states_and_provinces):\n",
    "                    states_and_provinces[line[i]] = set()\n",
    "                    states_and_provinces[line[i]].add(line[j])\n",
    "\n",
    "                else:\n",
    "                    states_and_provinces[line[i]].add(line[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Dutch names for countries\n",
    "\n",
    "countries = set()\n",
    "countries_file = open('..\\\\Data\\\\input\\\\countries.csv', newline='')\n",
    "reader = csv.reader(countries_file, delimiter=',', quotechar='\"')\n",
    "\n",
    "for line in reader:\n",
    "    countries.add(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Dutch names for world cities\n",
    "\n",
    "world_cities = []\n",
    "world_cities_file = open('..\\\\Data\\\\input\\\\worldcities.csv', newline='')\n",
    "reader = csv.reader(world_cities_file, delimiter=',', quotechar='\"')\n",
    "\n",
    "for line in reader:\n",
    "    if not(line[0] in world_cities):\n",
    "        world_cities.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Dutch names for Belgian communities\n",
    "\n",
    "belgian_communities = {}\n",
    "belgian_communities_file = open('..\\\\Data\\\\input\\\\belgian_communities.csv', newline='', encoding='utf-8')\n",
    "reader = csv.reader(belgian_communities_file, delimiter=',', quotechar='\"')\n",
    "\n",
    "for line in reader:\n",
    "    \n",
    "    if not(line[1].lower() in belgian_communities):\n",
    "        belgian_communities[line[1]] = set()\n",
    "        belgian_communities[line[1]].add(line[0])\n",
    "        \n",
    "    else:\n",
    "        belgian_communities[line[1]].add(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Dutch names for nationalities\n",
    "\n",
    "nationalities = {}\n",
    "nationalities_file = open('..\\\\Data\\\\input\\\\nationalities.csv', newline='')\n",
    "reader = csv.reader(nationalities_file, delimiter=',', quotechar='\"')\n",
    "\n",
    "for line in reader:\n",
    "    for i in range(0, 5):\n",
    "        for j in range(0, 5):\n",
    "            if i != j:\n",
    "                if not(line[i] in nationalities):\n",
    "                    nationalities[line[i]] = set()\n",
    "                    nationalities[line[i]].add(line[j])\n",
    "\n",
    "                else:\n",
    "                    nationalities[line[i]].add(line[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help function that takes as an input a tokenized text and takes as an output arrays containing the colors, days, ... it contains\n",
    "#this function can be used to calculate features concerning entities being present in atomic changes\n",
    "\n",
    "def entity_spotting(doc):\n",
    "    col = []\n",
    "    day = []\n",
    "    curr = []\n",
    "    mon = []\n",
    "    winds = []\n",
    "    states = []\n",
    "    countr = []\n",
    "    cit = []\n",
    "    belgian = []\n",
    "    nationality = []\n",
    "    \n",
    "    for token in doc:\n",
    "        text = token.text.lower()       \n",
    "        \n",
    "        #colors\n",
    "        if text in colors:\n",
    "            col.append(text)\n",
    "            col.append(colors[text])\n",
    "            \n",
    "        else:\n",
    "            found = [s for s in colors if s in text]\n",
    "           \n",
    "            for s in found:\n",
    "                col.append(s)\n",
    "                col.append(colors[s])\n",
    "            \n",
    "            \n",
    "        #days\n",
    "        if text in days:\n",
    "            day.append(text)\n",
    "            \n",
    "        else:\n",
    "            found = [s for s in days if s in text]\n",
    "           \n",
    "            for s in found:\n",
    "                day.append(s)\n",
    "        \n",
    "        \n",
    "        #currencies\n",
    "        if text in currencies:\n",
    "            curr.append(text)\n",
    "        \n",
    "        #months\n",
    "        if text in months:\n",
    "            mon.append(text)\n",
    "            \n",
    "        else:\n",
    "            found = [s for s in months if s in text]\n",
    "           \n",
    "            for s in found:\n",
    "                mon.append(s)\n",
    "            \n",
    "            \n",
    "        #wind directions\n",
    "        if text in wind_directions:\n",
    "            winds.append(text)\n",
    "            \n",
    "        else:\n",
    "            found = [s for s in wind_directions if s in text]\n",
    "           \n",
    "            for s in found:\n",
    "                winds.append(s)\n",
    "        \n",
    "        \n",
    "        #states\n",
    "        if text in states_and_provinces:\n",
    "            states.append(text)\n",
    "            \n",
    "        else:\n",
    "            stats = [st for st in states_and_provinces if st in text]\n",
    "           \n",
    "            for b in stats:\n",
    "                states.append(b)\n",
    "                \n",
    "                for statesje in states_and_provinces[b]:\n",
    "                    states.append(statesje)\n",
    "            \n",
    "            \n",
    "        #countries\n",
    "        if text in countries:\n",
    "            countr.append(text)\n",
    "           \n",
    "        \n",
    "        #world cities\n",
    "        if text in world_cities:\n",
    "            cit.append(text)\n",
    "            \n",
    "        #belgian communities\n",
    "        if text in belgian_communities:\n",
    "            belgian.append(text)\n",
    "            \n",
    "            for community in belgian_communities[text]:\n",
    "                belgian.append(community)\n",
    "            \n",
    "        #nationalities\n",
    "        if text in nationalities:\n",
    "            nationality.append(text)\n",
    "            \n",
    "            for nationalit in nationalities[text]:\n",
    "                nationality.append(nationalit)\n",
    "            \n",
    "    return col, day, curr, mon, winds, states, countr, cit, belgian, nationality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help function that takes two arrays as an input (typically containing entities of the same category, one for the original article version and one for the new article version, such as e.g. colors) \n",
    "#and gives as an output a number describing the extent to which entities were added and/or removed to/from the atomic change\n",
    "\n",
    "#information on the meaning of the values that these features can hold can be found in the input data model documentation \n",
    "\n",
    "def array_comparison(array1, array2, result):\n",
    "    if (len(array1) == 0) and (len(array2) == 0):\n",
    "        result.append(0)\n",
    "    \n",
    "    else:\n",
    "        if (array1 == array2):\n",
    "            result.append(1)\n",
    "            \n",
    "        else:\n",
    "            if set(array1).issubset(set(array2)):\n",
    "                result.append(2)\n",
    "                \n",
    "            else:\n",
    "                if set(array2).issubset(set(array1)):\n",
    "                    result.append(3)\n",
    "                    \n",
    "                else:\n",
    "                    if (len(array1) == 1) and (len(array2) == 1): \n",
    "                        result.append(4)\n",
    "                        \n",
    "                    else:\n",
    "                        result.append(5)\n",
    "                    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function calculating all features related to the spaCy model, as described in the input data model documentation\n",
    "\n",
    "def use_spacy(true, original_full, original_diff, new_full, new_diff, original_textpart, new_textpart,  \\\n",
    "                                                        original_changed_sentences, new_changed_sentences):\n",
    "\n",
    "    numbers_mapping = {'anderhalf': '1.5', 'anderhalve': '1.5', 'kwart': '0.25', 'drie kwart': '0.75', 'driekwart': '0.75',\n",
    "                       'de helft': '0.5', '1/2': '0.5', '1/2e': '0.5',\n",
    "                       'een derde': '0.3333333', 'één derde': '0.3333333', '1/3': '0.3333333', '1/3e': '0.3333333',\n",
    "                       'een vierde': '0.25', 'één vierde': '0.25', '1/4': '0.25', '1/4e': '0.25',\n",
    "                       'een vijfde': '0.2', 'één vijfde': '0.2', '1/5': '0.2', '1/5e': '0.2', \n",
    "                       'een zesde': '0.16666667', 'één zesde': '0.16666667', '1/6': '0.16666667', '1/6e': '0.16666667',\n",
    "                       'een zevende': '0.14286', 'één zevende': '0.14286', '1/7': '0.14286', '1/7e': '0.14286',\n",
    "                       'een achtste': '0.125', 'één achtste': '0.125', '1/8': '0.125', '1/8e': '0.125',\n",
    "                       'een negende': '0.1111111111', 'één negende': '0.1111111111', '1/9': '0.1111111111', '1/9e': '0.1111111111',\n",
    "                       'een tiende': '0.1', 'één tiende': '0.1', '1/10': '0.1', '1/10e': '0.1',\n",
    "                       'een zestiende': '0.0625', 'één zestiende': '0.0625','1/16': '0.0625', '1/16e': '0.0625',\n",
    "                       'nul': '0', 'nulde': '0', '0de': '0',\n",
    "                       'één': '1', 'een': '1', 'eén': '1', 'eerste': '1', '1ste': '1', '1e': '1',\n",
    "                       'twee': '2','tweede': '2', '2de': '2', '2e': '2', 'tweeën': '2',\n",
    "                       'drie': '3', 'derde': '3', '3de': '3', '3e': '3', 'drieën': '3',\n",
    "                       'vier': '4', 'vierde': '4', '4de': '4', '4e': '4', 'vieren': '4',\n",
    "                       'vijf': '5', 'vijfde': '5', '5de': '5', '5e': '5', 'vijven': '5',\n",
    "                       'zes': '6', 'zesde': '6', '6de': '6', '6e': '6', 'zessen': '6',\n",
    "                      'zeven': '7', 'zevende': '7', '7de': '7', '7e': '7', 'zevenen': '7',\n",
    "                       'acht': '8', 'achtste': '8', '8ste': '8', '8e': '8', 'achten': '8',\n",
    "                       'negen': '9', 'negende': '9', '9de': '9', '9e': '9', 'negenen': '9',\n",
    "                       'tien': '10', 'tiende': '10', '10de': '10', '10e': '10', 'tienen': '10',\n",
    "                       'elf': '11', 'elfde': '11', '11de': '11', '11e': '11', 'elven': '11',\n",
    "                       'twaalf': '12', 'twaalfde': '12', '12de': '12', '12e': '12', 'twaalven': '12',\n",
    "                       'dertien': '13', 'dertiende': '13', '13de': '13', '13e': '13', 'dertienen': '13',\n",
    "                      'veertien': '14', 'veertiende': '14', '14de': '14', '14e': '14', 'veertienen': '14',\n",
    "                       'vijftien': '15', 'vijftiende': '15', '15de': '15', '15e': '15', 'vijftienen': '15',\n",
    "                       'zestien': '16', 'zestiende': '16', '16de': '16', '16e': '16', 'zestienen': '16',\n",
    "                       'zeventien': '17', 'zeventiende': '17', '17de': '17', '17e': '17', 'zeventienen': '17',\n",
    "                       'achttien': '18', 'achttiende': '18', '18de': '18', '18e': '18', 'achttienen': '18',\n",
    "                      'negentien': '19', 'negentiende': '19', '19de': '19', '19e': '19', 'negentienen': '19',\n",
    "                       'twintig': '20', 'twintigste': '20', '20ste': '20', '20e': '20',\n",
    "                       'dertig': '30', 'dertigste': '30', '30ste': '30', '30e': '30',\n",
    "                       'veertig': '40', 'veertigste': '40', '40ste': '40', '40e': '40',\n",
    "                       'vijftig': '50', 'vijftigste': '50', '50ste': '50', '50e': '50',\n",
    "                       'zestig': '60', 'zestigste': '60', '60ste': '60', '60e': '60',\n",
    "                       'zeventig': '70','zeventigste': '70', '70ste': '70', '70e': '70',\n",
    "                       'tachtig': '80', 'tachtigste': '80', '80ste': '80', '80e': '80',\n",
    "                       'negentig': '90', 'negentigste': '90', '90ste': '90', '90e': '90',\n",
    "                       'honderd': '100', 'honderdste': '100', '100ste': '100', '100e': '100',\n",
    "                       'tweehonderd': '200', 'tweehonderdste': '200', '20ste': '200', '200e': '200',\n",
    "                       'driehonderd': '300', 'driehonderdste': '300', '300ste': '300', '300e': '300',\n",
    "                       'vierhonderd': '400', 'vierhonderdste': '400', '400ste': '400', '400e': '400',\n",
    "                      'vijfhonderd': '500', 'vijfhonderdste': '500', '500ste': '500', '500e': '500',\n",
    "                       'zeshonderd': '600', 'zeshonderdste': '600', '600ste': '600', '600e': '600',\n",
    "                       'zevenhonderd': '700', 'zevenhonderdste': '700', '700ste': '700', '700e': '700',\n",
    "                       'achthonderd': '800', 'achthonderdste': '800', '800ste': '800', '800e': '800',\n",
    "                       'negenhonderd': '900', 'negenhonderdste': '900', '900ste': '900', '900e': '900',\n",
    "                       'duizend': '1000', 'duizendste': '1000', '1000ste': '1000', '1000e': '1000', \n",
    "                       'tweeduizend': '2000', 'tweeduizendste': '2000', '2000ste': '2000', '2000e': '2000', \n",
    "                       'drieduizend': '3000', 'drieduizendste': '3000', '3000ste': '3000', '3000e': '3000', \n",
    "                       'vierduizend': '4000', 'vierduizendste': '4000', '4000ste': '4000', '4000e': '4000', \n",
    "                       'vijfduizend': '5000', 'vijfduizendste': '5000', '5000ste': '5000', '5000e': '5000', \n",
    "                       'zesduizend': '6000', 'zesduizendste': '6000', '6000ste':  '6000', '6000e': '6000', \n",
    "                       'zevenduizend': '7000', 'zevenduizendste': '7000', '7000ste': '7000', '7000e': '7000', \n",
    "                       'achtduizend': '8000', 'achtduizendste': '8000', '8000ste': '8000', '8000e': '8000', \n",
    "                       'negenduizend': '9000', 'negenduizendste': '9000', '9000ste': '9000', '9000e': '9000', \n",
    "                       'tienduizend': '10000', 'tienduizendste': '100000', '10000ste': '10000', '100000e': '10000',\n",
    "                       'honderdduizend': '100000', 'honderdduizendste': '100000', '100000ste': '100000', '100000e': '100000',\n",
    "                       'tweehonderdduizend': '200000', 'tweehonderdduizendste': '200000', '200000ste': '200000', '200000e': '200000',\n",
    "                       'driehonderdduizend': '300000', 'driehonderdduizendste': '300000', '300000ste': '300000', '300000e': '300000',\n",
    "                       'vierhonderdduizend': '400000', 'vierhonderdduizendste': '400000', '400000ste': '400000', '400000e': '400000',\n",
    "                       'vijfhonderdduizend': '500000', 'vijfhonderdduizendste': '400000', '500000ste': '500000', '500000e': '500000',\n",
    "                       'zeshonderdduizend': '600000', 'zeshonderdduizendste': '600000', '600000ste': '600000', '600000e': '600000',\n",
    "                       'zevenhonderdduizend': '700000', 'zevenhonderdduizendste': '700000', '700000ste': '700000', '700000e': '700000',\n",
    "                       'achthonderdduizend': '800000', 'achthonderdduizendste': '800000', '800000ste': '800000', '800000e': '800000',\n",
    "                       'negenhonderdduizend': '900000', 'negenhonderdduizendste': '900000', '900000ste': '900000', '900000e': '900000',\n",
    "                       'één miljoen': '1000000', 'een miljoen': '1000000', '1 miljoen': '1000000',\n",
    "                       'twee miljoen': '1000000', '2 miljoen': '1000000',\n",
    "                       'drie miljoen': '1000000', '3 miljoen': '1000000',\n",
    "                       'vier miljoen': '1000000', '4 miljoen': '1000000',\n",
    "                       'vijf miljoen': '1000000', '5 miljoen': '1000000',\n",
    "                       'zes miljoen': '1000000', '6 miljoen': '1000000',\n",
    "                       'zeven miljoen': '1000000', '7 miljoen': '1000000',\n",
    "                       'acht miljoen': '1000000', '8 miljoen': '1000000',\n",
    "                       'negen miljoen': '1000000', '9 miljoen': '1000000',\n",
    "                       'tien miljoen': '1000000', '10 miljoen': '1000000',\n",
    "                        'één miljard': '1000000', 'een miljard': '1000000', '1 miljard': '1000000',\n",
    "                       'twee miljard': '1000000', '2 miljard': '1000000',\n",
    "                       'drie miljard': '1000000', '3 miljard': '1000000',\n",
    "                       'vier miljard': '1000000', '4 miljard': '1000000',\n",
    "                       'vijf miljard': '1000000', '5 miljard': '1000000',\n",
    "                       'zes miljard': '1000000', '6 miljard': '1000000',\n",
    "                       'zeven miljard': '1000000', '7 miljard': '1000000',\n",
    "                       'acht miljard': '1000000', '8 miljard': '1000000',\n",
    "                       'negen miljard': '1000000', '9 miljard': '1000000',\n",
    "                       'tien miljard': '1000000', '10 miljard': '1000000',\n",
    "                      }\n",
    "    \n",
    "    punct_mapping = {\"'\": '\"', '“': '\"', '”': '\"', '`': '\"', '‘': '\"', '’': '\"'}\n",
    "    \n",
    "    pos_tags = ['ADV', 'NOUN', '.', ',', '\"', '(', ':', '-', '...', 'PUNCT', 'X', 'PROPN', 'PRON', \n",
    "                'DET', 'SCONJ', 'SPACE', 'SYM', 'NUM', 'ADP', 'INTJ', 'AUX', 'inf', 'pv_verl_ev', 'pv_verl_mv', 'pv_tgw_ev',\n",
    "                'pv_tgw_mv',\n",
    "                'od_prenom', 'od_nom', 'od_postnom', 'od_vrij', 'vd_vrij', 'vd_prenom', 'vd_postnom', 'vd_nom', 'VERB', 'CCONJ',\n",
    "                'adj_sup', 'adj_comp', 'adj_basis']\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    doc_original = nlp(original_full.strip())\n",
    "    doc_new = nlp(new_full.strip())\n",
    "    \n",
    "    \n",
    "    doc_diff_original = nlp(original_diff.strip())\n",
    "    doc_diff_new = nlp(new_diff.strip())\n",
    "    \n",
    "    \n",
    "    total_words = len(doc_diff_original) + len(doc_diff_new)\n",
    "    total_stop_words = 0\n",
    "    \n",
    "    for token in doc_diff_original:\n",
    "        if token.text.lower() in stop_words:\n",
    "            total_stop_words = total_stop_words + 1\n",
    "            \n",
    "    for token in doc_diff_new:\n",
    "        if token.text.lower() in stop_words:\n",
    "            total_stop_words = total_stop_words + 1\n",
    "            \n",
    "    if total_words > 0:\n",
    "        result.append(float(total_stop_words)/total_words)\n",
    "    else:\n",
    "        result.append(0)\n",
    "\n",
    "    \n",
    "    ent_original = 0\n",
    "    ent_new = 0\n",
    "    \n",
    "    startindex = 0\n",
    "    \n",
    "    numbers_original = set()\n",
    "    numbers_new = set()\n",
    "    \n",
    "    person_original = set()\n",
    "    person_new = set()\n",
    "    \n",
    "    date_original = set()\n",
    "    date_new = set()\n",
    "    \n",
    "    #ORIGINAL ENTS\n",
    "    \n",
    "    temp_original_diff = original_diff\n",
    "    temp_new_diff = new_diff\n",
    "    \n",
    "    other_ent_occurences_in_new = True\n",
    "    \n",
    "    nrs = re.findall(r'\\d+', original_diff)\n",
    "    for nr in nrs:\n",
    "        numbers_original.add(nr)\n",
    "    \n",
    "\n",
    "    for ent in doc_original.ents:\n",
    "        ents_tokenized = word_tokenize(ent.text)\n",
    "        \n",
    "        for ent_tok in ents_tokenized:\n",
    "            index =temp_original_diff.find(ent_tok)\n",
    "        \n",
    "            if index >= 0:\n",
    "                if ent.text.lower() in numbers_mapping:\n",
    "                    numbers_original.add(numbers_mapping[ent.text.lower()])\n",
    "                else:\n",
    "                    if ent_tok.lower() in numbers_mapping:\n",
    "                        numbers_original.add(numbers_mapping[ent_tok.lower()])\n",
    "                    else:\n",
    "                        if (ent.label_ == 'CARDINAL') or (ent.label_ == 'ORDINAL') \\\n",
    "                        or (ent.label_ == 'PERCENT') or (ent.label_ == 'TIME'):\n",
    "                            \n",
    "                            if any(map(str.isdigit, ent_tok.lower())):\n",
    "                                numbers_original.add(ent_tok.lower())\n",
    "                        else:\n",
    "\n",
    "                            if (ent.label_ == 'DATE') and ent_tok.isnumeric():\n",
    "                                date_original.add(ent_tok.lower())\n",
    "\n",
    "                            else:\n",
    "                                if (ent.label_ == 'PERSON'):\n",
    "                                    person_original.add(ent_tok.lower())\n",
    "\n",
    "                                else:\n",
    "                                    ent_original = ent_original + 1 \n",
    "                \n",
    "                startindex = startindex + len(ent_tok)\n",
    "                temp_original_diff = temp_original_diff[startindex:]\n",
    "                    \n",
    "            #look if entity is present in same text part at different position\n",
    "            if not(ent_tok) in new_textpart:\n",
    "                other_ent_occurences_in_new = False\n",
    "            \n",
    "    if len(doc_diff_original) == 0:\n",
    "        result.append(0)\n",
    "        \n",
    "    else:\n",
    "        result.append(float(ent_original)/len(doc_diff_original))\n",
    "    \n",
    "    result.append(len(doc_diff_original))\n",
    "    \n",
    "    #ORIGINAL TOKENS\n",
    "    original_pos_counts = {}\n",
    "    original_diff_lookup = [token.text for token in doc_diff_original]\n",
    "    \n",
    "    original_spelling_ok = 1\n",
    "    \n",
    "    original_double_word = False\n",
    "    \n",
    "    \n",
    "    temp_original_diff = original_diff\n",
    "    temp_new_diff = new_diff\n",
    "\n",
    "    for (i, token) in enumerate(doc_original):\n",
    "        index = temp_original_diff.find(token.text)\n",
    "        \n",
    "        if index >= 0:\n",
    "            temp_original_diff = temp_original_diff[len(token.text):]\n",
    "\n",
    "    \n",
    "        if (token.pos_ != 'PUNCT') and (index >= 0) and ((new_full.find(token.text) >= 0) or len(new_full) == 0) and (temp_original_diff.find(token.text) < 0) and (((i > 0) and token.text == doc_original[i - 1].text) or ((i < len(doc_original) - 1) and (token.text == doc_original[i + 1].text))):\n",
    "            original_double_word = True\n",
    "        \n",
    "        if (len(original_diff_lookup) > 0) and (token.text == original_diff_lookup[0]):\n",
    "            \n",
    "            pos = token.pos_\n",
    "            \n",
    "            if token.pos_ == 'PUNCT':\n",
    "                if token.text == '.' or  token.text == '!' or token.text == '?':\n",
    "                    pos = '.'\n",
    "                \n",
    "                else: \n",
    "                    if token.text in punct_mapping or token.text == '\"':\n",
    "                        pos = '\"'\n",
    "                    \n",
    "                    else:\n",
    "                        if token.text == ',':\n",
    "                            pos = ','\n",
    "                            \n",
    "                        else:\n",
    "                            if '(' in token.text or ')' in token.text:\n",
    "                                pos = '('\n",
    "                                \n",
    "                            else:\n",
    "                                if ':' in token.text:\n",
    "                                    pos = ':'\n",
    "                                    \n",
    "                                else:\n",
    "                                    if token.text == '-' or token.text == ';' or token.text == '–':\n",
    "                                        pos = '-'\n",
    "                                        \n",
    "                                    else:\n",
    "                                        \n",
    "                                        if token.text == '..' or token.text == '...' or token.text == '....':\n",
    "                                            pos = '...'\n",
    "                                            \n",
    "            if token.pos_ == 'ADJ':\n",
    "                if ('sup' in token.tag_):\n",
    "                    pos = 'adj_sup'\n",
    "                    \n",
    "                else:\n",
    "                    if ('comp' in token.tag_):\n",
    "                        pos = 'adj_comp'\n",
    "                    \n",
    "                    else:\n",
    "                        pos = 'adj_basis'\n",
    "                \n",
    "            if token.pos_ == 'VERB'  or token.pos == 'AUX':\n",
    "                if 'inf|' in token.tag_:\n",
    "                    pos = 'inf'\n",
    "                \n",
    "                else:\n",
    "                    if 'pv|' in token.tag_:\n",
    "                        if '|tgw|' in token.tag_:\n",
    "                            if ('|ev' in token.tag_) or ('|met-t' in token.tag_):\n",
    "                                pos = 'pv_tgw_ev'\n",
    "                            \n",
    "                            else:\n",
    "                                pos = 'pv_tgw_mv'\n",
    "                        \n",
    "                        if '|verl|' in token.tag_:\n",
    "                            if ('|ev' in token.tag_) or ('|met-t' in token.tag_):\n",
    "                                pos = 'pv_verl_ev'\n",
    "                            \n",
    "                            else:\n",
    "                                pos = 'pv_verl_mv'\n",
    "                    \n",
    "                    else:\n",
    "                        if 'od|' in token.tag_:\n",
    "                            if '|prenom|' in token.tag_:\n",
    "                                pos = 'od_prenom'\n",
    "                                \n",
    "                            if '|postnom|' in token.tag_:\n",
    "                                pos = 'od_postnom'\n",
    "                                \n",
    "                            if '|nom|' in token.tag_:\n",
    "                                pos = 'od_nom'\n",
    "                                \n",
    "                            if '|vrij|' in token.tag_:\n",
    "                                pos = 'od_vrij'\n",
    "                                \n",
    "                        else:\n",
    "                            if 'vd|' in token.tag_:\n",
    "                                if '|prenom|' in token.tag_:\n",
    "                                    pos = 'od_prenom'\n",
    "\n",
    "                                if '|postnom|' in token.tag_:\n",
    "                                    pos = 'od_postnom'\n",
    "\n",
    "                                if '|nom|' in token.tag_:\n",
    "                                    pos = 'od_nom'\n",
    "\n",
    "                                if '|vrij|' in token.tag_:\n",
    "                                    pos = 'od_vrij'\n",
    "        \n",
    "\n",
    "            if pos in original_pos_counts:\n",
    "                original_pos_counts[pos] = original_pos_counts[pos] + 1\n",
    "            else:\n",
    "                original_pos_counts[pos] = 1\n",
    "                \n",
    "                \n",
    "            original_diff_lookup = original_diff_lookup[1:]\n",
    "            \n",
    "    original_pos_counts['SPACE'] = original_diff.count(' ')\n",
    "            \n",
    "    original_token_sum = sum(original_pos_counts.values())\n",
    "    \n",
    "    if original_token_sum  - original_pos_counts['SPACE'] == 1:\n",
    "        token = doc_diff_original[0]\n",
    "        if not(token.text.lower() in dutch_words) and token.pos_ != 'PUNCT' and token.pos_ != 'SYM' and token.pos_ != 'NUM':\n",
    "                original_spelling_ok = 0\n",
    "    \n",
    "    if original_token_sum == 0:\n",
    "        \n",
    "        for i in range(0, 39):\n",
    "            result.append(0)\n",
    "        \n",
    "    else:\n",
    "        for pos_tag in pos_tags:\n",
    "            if pos_tag in original_pos_counts:\n",
    "                result.append(float(original_pos_counts[pos_tag])/original_token_sum)\n",
    "                \n",
    "            else:\n",
    "                result.append(0)          \n",
    "    \n",
    "    #NEW ENTS\n",
    "    startindex = 0\n",
    "    \n",
    "    temp_original_diff = original_diff\n",
    "    temp_new_diff = new_diff\n",
    "    \n",
    "    other_ent_occurences_in_original = True\n",
    "    \n",
    "    nrs = re.findall(r'\\d+', new_diff)\n",
    "    for nr in nrs:\n",
    "        numbers_new.add(nr)\n",
    "    \n",
    "    for ent in doc_new.ents:\n",
    "        ents_tokenized = word_tokenize(ent.text)\n",
    "        \n",
    "        for ent_tok in ents_tokenized:\n",
    "            index =temp_new_diff.find(ent_tok)\n",
    "        \n",
    "            if index >= 0:\n",
    "                if ent.text.lower() in numbers_mapping:\n",
    "                    numbers_new.add(numbers_mapping[ent.text.lower()])\n",
    "                else:\n",
    "                    if ent_tok.lower() in numbers_mapping:\n",
    "                        numbers_new.add(numbers_mapping[ent_tok.lower()])\n",
    "                    else:\n",
    "                        if (ent.label_ == 'CARDINAL') or (ent.label_ == 'ORDINAL') \\\n",
    "                        or (ent.label_ == 'PERCENT') or (ent.label_ == 'TIME'):\n",
    "                           \n",
    "                            if any(map(str.isdigit, ent_tok.lower())):\n",
    "                                numbers_new.add(ent_tok.lower())\n",
    "                        else:\n",
    "\n",
    "                            if (ent.label_ == 'DATE') and ent_tok.isnumeric():\n",
    "                                date_new.add(ent_tok.lower())\n",
    "\n",
    "                            else:\n",
    "                                if (ent.label_ == 'PERSON'):\n",
    "                                    person_new.add(ent_tok.lower())\n",
    "\n",
    "                                else:\n",
    "                                    ent_new = ent_new + 1 \n",
    "                \n",
    "                startindex = startindex + len(ent_tok)\n",
    "                temp_new_diff = temp_new_diff[startindex:]\n",
    "                \n",
    "            #look if entity is present in same text part at different position\n",
    "            if not(ent_tok) in original_textpart:\n",
    "                other_ent_occurences_in_original = False\n",
    "    \n",
    "    if len(doc_diff_new) == 0:\n",
    "        result.append(0)\n",
    "        \n",
    "    else:\n",
    "        result.append(float(ent_new)/len(doc_diff_new))\n",
    "        \n",
    "    result.append(len(doc_diff_new))\n",
    "    \n",
    "    #NEW TOKENS\n",
    "    new_pos_counts = {}\n",
    "    new_diff_lookup = [token.text for token in doc_diff_new]\n",
    "    \n",
    "    new_spelling_ok = 1\n",
    "    \n",
    "    new_double_word = False\n",
    "    \n",
    "    temp_original_diff = original_diff\n",
    "    temp_new_diff = new_diff\n",
    "    \n",
    "    for i, token in enumerate(doc_new):\n",
    "        \n",
    "        index = new_diff.find(token.text)\n",
    "        \n",
    "        if index >= 0:\n",
    "            temp_new_diff = temp_new_diff[len(token.text):]\n",
    "        \n",
    "        if (token.pos_ != 'PUNCT') and (index >= 0) and ((original_full.find(token.text) >= 0) or len(original_full) == 0) and (temp_new_diff.find(token.text) < 0) and (((i > 0) and token.text == doc_new[i - 1].text) or ((i < len(doc_new) - 1) and (token.text == doc_new[i + 1].text))):\n",
    "            new_double_word = True\n",
    "\n",
    "        if (len(new_diff_lookup) > 0) and (token.text == new_diff_lookup[0]):\n",
    "            \n",
    "            pos = token.pos_\n",
    "            \n",
    "            if token.pos_ == 'PUNCT':\n",
    "                if token.text == '.' or  token.text == '!' or token.text == '?':\n",
    "                    pos = '.'\n",
    "                \n",
    "                else: \n",
    "                    if token.text in punct_mapping or token.text == '\"':\n",
    "                        pos = '\"'\n",
    "                    \n",
    "                    else:\n",
    "                        if token.text == ',':\n",
    "                            pos = ','\n",
    "                            \n",
    "                        else:\n",
    "                            if '(' in token.text or ')' in token.text:\n",
    "                                pos = '('\n",
    "                                \n",
    "                            else:\n",
    "                                if ':' in token.text:\n",
    "                                    pos = ':'\n",
    "                                    \n",
    "                                else:\n",
    "                                    if token.text == '-' or token.text == ';' or token.text == '–':\n",
    "                                        pos = '-'\n",
    "                                        \n",
    "                                    else:\n",
    "                                        \n",
    "                                        if token.text == '..' or token.text == '...' or token.text == '....':\n",
    "                                            pos = '...'\n",
    "                                            \n",
    "            if token.pos_ == 'ADJ':\n",
    "                if ('sup' in token.tag_):\n",
    "                    pos = 'adj_sup'\n",
    "                    \n",
    "                else:\n",
    "                    if ('comp' in token.tag_):\n",
    "                        pos = 'adj_comp'\n",
    "                    \n",
    "                    else:\n",
    "                        pos = 'adj_basis'\n",
    "                        \n",
    "            if token.pos_ == 'VERB' or token.pos == 'AUX':\n",
    "                if 'inf|' in token.tag_:\n",
    "                    pos = 'inf'\n",
    "                \n",
    "                else:\n",
    "                    if 'pv|' in token.tag_:\n",
    "                        if '|tgw|' in token.tag_:\n",
    "                            if ('|ev' in token.tag_) or ('|met-t' in token.tag_):\n",
    "                                pos = 'pv_tgw_ev'\n",
    "                            \n",
    "                            else:\n",
    "                                pos = 'pv_tgw_mv'\n",
    "                        \n",
    "                        if '|verl|' in token.tag_:\n",
    "                            if ('|ev' in token.tag_) or ('|met-t' in token.tag_):\n",
    "                                pos = 'pv_verl_ev'\n",
    "                            \n",
    "                            else:\n",
    "                                pos = 'pv_verl_mv'\n",
    "                    \n",
    "                    else:\n",
    "                        if 'od|' in token.tag_:\n",
    "                            if '|prenom|' in token.tag_:\n",
    "                                pos = 'od_prenom'\n",
    "                                \n",
    "                            if '|postnom|' in token.tag_:\n",
    "                                pos = 'od_postnom'\n",
    "                                \n",
    "                            if '|nom|' in token.tag_:\n",
    "                                pos = 'od_nom'\n",
    "                                \n",
    "                            if '|vrij|' in token.tag_:\n",
    "                                pos = 'od_vrij'\n",
    "                                \n",
    "                        else:\n",
    "                            if 'vd|' in token.tag_:\n",
    "                                if '|prenom|' in token.tag_:\n",
    "                                    pos = 'od_prenom'\n",
    "\n",
    "                                if '|postnom|' in token.tag_:\n",
    "                                    pos = 'od_postnom'\n",
    "\n",
    "                                if '|nom|' in token.tag_:\n",
    "                                    pos = 'od_nom'\n",
    "\n",
    "                                if '|vrij|' in token.tag_:\n",
    "                                    pos = 'od_vrij'\n",
    "            \n",
    "            if pos in new_pos_counts:\n",
    "                new_pos_counts[pos] = new_pos_counts[pos] + 1\n",
    "            else:\n",
    "                new_pos_counts[pos] = 1\n",
    "            \n",
    "            new_diff_lookup = new_diff_lookup[1:]\n",
    "            \n",
    "            \n",
    "    new_pos_counts['SPACE'] = new_diff.count(' ')\n",
    "    new_token_sum = sum(new_pos_counts.values())\n",
    "    \n",
    "    if new_token_sum - new_pos_counts['SPACE'] == 1:\n",
    "        token = doc_diff_new[0]\n",
    "        if not(token.text.lower() in dutch_words) and token.pos_ != 'PUNCT' and token.pos_ != 'SYM' and token.pos_ != 'NUM':\n",
    "                new_spelling_ok = 0\n",
    "    \n",
    "    else:\n",
    "        original_spelling_ok = 1\n",
    "    \n",
    "    if new_token_sum == 0:\n",
    "        \n",
    "        for i in range(0, 39):\n",
    "            result.append(0)\n",
    "        \n",
    "    else:\n",
    "        for pos_tag in pos_tags:\n",
    "            if pos_tag in new_pos_counts:\n",
    "                result.append(float(new_pos_counts[pos_tag])/new_token_sum)\n",
    "\n",
    "            else:\n",
    "                result.append(0)\n",
    "    \n",
    "    if original_double_word and not(new_double_word):\n",
    "        result.append(1)\n",
    "    else:\n",
    "        result.append(0)\n",
    "    \n",
    "    #FINAL COMPARISON\n",
    "    \n",
    "    temp_original_diff = original_diff\n",
    "    temp_new_diff = new_diff\n",
    "    \n",
    "    temp_original_full = original_full\n",
    "    temp_new_full = new_full\n",
    "    \n",
    "    for token in doc_diff_original:\n",
    "        if token.text.lower() in numbers_mapping:\n",
    "            \n",
    "            while token.text in temp_original_diff:\n",
    "                temp_original_diff = temp_original_diff.replace(token.text, numbers_mapping[token.text.lower()])\n",
    "                temp_original_full = temp_original_full.replace(token.text, numbers_mapping[token.text.lower()])\n",
    "                \n",
    "        if token.text in punct_mapping:\n",
    "            \n",
    "            while token.text in temp_original_diff:\n",
    "                temp_original_diff = temp_original_diff.replace(token.text, punct_mapping[token.text])\n",
    "                temp_original_full = temp_original_full.replace(token.text, punct_mapping[token.text])\n",
    "                \n",
    "    for token in doc_diff_new:\n",
    "        if token.text.lower() in numbers_mapping:\n",
    "            \n",
    "            while token.text in temp_new_diff:\n",
    "                temp_new_diff = temp_new_diff.replace(token.text, numbers_mapping[token.text.lower()])\n",
    "                temp_new_full = temp_new_full.replace(token.text, numbers_mapping[token.text.lower()])\n",
    "                \n",
    "        if token.text in punct_mapping:\n",
    "            \n",
    "            while token.text in temp_new_diff:\n",
    "                temp_new_diff = temp_new_diff.replace(token.text, punct_mapping[token.text])\n",
    "                temp_new_full = temp_new_full.replace(token.text, punct_mapping[token.text])\n",
    "    \n",
    "    if temp_original_diff == temp_new_diff:\n",
    "        result.append(1)\n",
    "        \n",
    "        doc_temp_original_full = [token.text for token in nlp(temp_original_full)]\n",
    "        doc_temp_new_full = [token.text for token in nlp(temp_new_full)]\n",
    "        \n",
    "        bool2 = True\n",
    "        \n",
    "        for token in nlp(temp_original_diff):\n",
    "            try:\n",
    "                index_original = doc_temp_original_full.index(token.text)\n",
    "            except:\n",
    "                index_original = 0\n",
    "            \n",
    "            try:\n",
    "                index_new = doc_temp_new_full.index(token.text)\n",
    "            except:\n",
    "                index_new = 0\n",
    "            \n",
    "            if ((index_original > 0) and (index_new > 0) and (doc_temp_original_full[index_original - 1]  != doc_temp_new_full[index_new - 1])) \\\n",
    "            or ((index_original < len(doc_temp_original_full)-1) and (index_new < len(doc_temp_new_full)-1) and (doc_temp_original_full[index_original + 1]  != doc_temp_new_full[index_new + 1])) \\\n",
    "            or ((index_original == 0) and (index_new != 0)) or ((index_original == len(doc_temp_original_full) -1) and (index_new != len(doc_temp_new_full) - 1)) \\\n",
    "            or ((index_original != 0) and (index_new == 0)) or ((index_original != len(doc_temp_original_full) -1) and (index_new == len(doc_temp_new_full) - 1)):\n",
    "                bool2 = False\n",
    "                break\n",
    "        if bool2 == True:\n",
    "            result.append(1)    \n",
    "        else:\n",
    "            result.append(0)\n",
    "        \n",
    "    else:\n",
    "        result.append(0)\n",
    "        result.append(0)\n",
    "        \n",
    "        \n",
    "    #look at the type word before and after diff\n",
    "    original_tokens = [token.text for token in doc_diff_original]\n",
    "    \n",
    "    if (len(original_tokens) > 0):\n",
    "        first_token_original = original_tokens[0]\n",
    "        last_token_original = original_tokens[len(original_tokens) - 1]\n",
    "    \n",
    "    new_tokens = [token.text for token in doc_diff_new]\n",
    "    if (len(new_tokens) > 0):\n",
    "        first_token_new = new_tokens[0]\n",
    "        last_token_new = new_tokens[len(new_tokens) - 1]\n",
    "    \n",
    "    first = 'other'\n",
    "    last = 'other'\n",
    "    \n",
    "    if (original_diff == ',') and (new_diff == ''):\n",
    "    \n",
    "        if (len(original_tokens) > 0):\n",
    "            for i, token in enumerate(doc_original):\n",
    "                if (token.text == first_token_original) and first_token_original ==',' and (i > 0) and (first == 'other'):\n",
    "                    first = doc_original[i - 1].pos_\n",
    "\n",
    "                if (token.text == last_token_original) and last_token_original ==',' and (i < len(doc_original) - 1) and (last == 'other'):\n",
    "                    last = doc_original[i + 1].pos_\n",
    "    \n",
    "    if (original_diff == '') and (new_diff == ','):\n",
    "        if (len(new_tokens) > 0):\n",
    "            for i, token in enumerate(doc_new):\n",
    "                if (token.text == first_token_new) and first_token_new ==',' and (i > 0) and (first =='other'):\n",
    "                    first = doc_new[i - 1].pos_\n",
    "\n",
    "                if (token.text == last_token_new)  and last_token_new ==',' and (i < len(doc_new) - 1) and (last == 'other'):\n",
    "                    last = doc_new[i + 1].pos_\n",
    "        \n",
    "    result.append(first)\n",
    "    result.append(last)\n",
    "    \n",
    "    result.append(original_spelling_ok)\n",
    "    result.append(new_spelling_ok)\n",
    "    \n",
    "    #one edit change\n",
    "    if (new_token_sum - new_pos_counts['SPACE'] == 1) and (original_token_sum - original_pos_counts['SPACE'] == 1):\n",
    "        if ((len(new_diff) == len(original_diff) - 1) and (new_diff in original_diff)) \\\n",
    "        or ((len(original_diff) == len(new_diff) - 1) and (original_diff in new_diff)) \\\n",
    "        or set(sorted(original_diff)) == set(sorted(new_diff)):\n",
    "            result.append(1)\n",
    "            \n",
    "        else: result.append(0)\n",
    "            \n",
    "    else:\n",
    "        result.append(0)\n",
    "        \n",
    "    #comparison of numbers\n",
    "    succeeded_original = True\n",
    "    succeeded_new = True\n",
    "    original_number = None\n",
    "    new_number = None\n",
    "    \n",
    "    if (new_token_sum - new_pos_counts['SPACE'] == 1) and (original_token_sum - original_pos_counts['SPACE'] == 1):\n",
    "        try:\n",
    "            original_number = float(original_diff.replace(' ', ''))\n",
    "            \n",
    "        except Exception as e:\n",
    "            succeeded_original = False\n",
    "            \n",
    "        try:\n",
    "            new_number = float(new_diff.replace(' ', ''))\n",
    "            \n",
    "        except Exception as e:\n",
    "            succeeded_new = False\n",
    "            \n",
    "        if (succeeded_original == True) and (succeeded_new == True):\n",
    "            result.append(compare_numbers(original_number, new_number))\n",
    "                \n",
    "        else:\n",
    "            if (succeeded_original == False) and (succeeded_new == True):\n",
    "                if original_diff.replace(' ', '').lower() in numbers_mapping:\n",
    "                    result.append(compare_numbers(float(numbers_mapping[original_diff.replace(' ', '').lower()]), new_number))\n",
    "                    \n",
    "                else:\n",
    "                    result.append(0)\n",
    "                    \n",
    "            else:\n",
    "                if (succeeded_original == True) and (succeeded_new == False):\n",
    "                    if new_diff.replace(' ', '').lower() in numbers_mapping:\n",
    "                        already_done = True\n",
    "                        result.append(compare_numbers(original_number, float(numbers_mapping[new_diff.replace(' ', '').lower()])))\n",
    "\n",
    "                    else:\n",
    "                        result.append(0)\n",
    "                        \n",
    "                else:\n",
    "                    if (succeeded_original == False) and (succeeded_original == False):\n",
    "                        if (original_diff.replace(' ', '').lower() in numbers_mapping) and (new_diff.replace(' ', '').lower() in numbers_mapping):\n",
    "                            result.append(compare_numbers(float(numbers_mapping[original_diff.replace(' ', '').lower()]), float(numbers_mapping[new_diff.replace(' ', '').lower()])))                      \n",
    "            \n",
    "                        else: \n",
    "                            result.append(0)\n",
    "    else:\n",
    "        if (len(numbers_original) == 0) and (len(numbers_new) == 0):\n",
    "            result.append(0)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if (numbers_original == numbers_new) and (len(numbers_original) > 0):\n",
    "                result.append(2)\n",
    "\n",
    "            else:\n",
    "                numbers_o = set()\n",
    "                numbers_n = set()\n",
    "                others = False\n",
    "\n",
    "                for token in numbers_original:\n",
    "                    token = token.replace(',', '.')\n",
    "                    try:\n",
    "                        token_o = float(token)\n",
    "\n",
    "                    except:\n",
    "                        if token.lower() in numbers_mapping:\n",
    "                            token_o = float(numbers_mapping[token_o.lower()])\n",
    "                        else:\n",
    "                            token_o = None\n",
    "                            others = True\n",
    "\n",
    "                    if not(token_o is None):\n",
    "                        numbers_o.add(token_o)\n",
    "\n",
    "                for token in numbers_new:\n",
    "                    token = token.replace(',', '.')\n",
    "                    try:\n",
    "                        token_n = float(token)\n",
    "\n",
    "                    except:\n",
    "                        if token.lower() in numbers_mapping:\n",
    "                            token_n = float(numbers_mapping[token_n.lower()])\n",
    "                        else:\n",
    "                            token_n = None\n",
    "                            others = True\n",
    "\n",
    "                    if not(token_n is None):\n",
    "                        numbers_n.add(token_n)\n",
    "\n",
    "                if (numbers_o == numbers_n) and (len(numbers_original) > 0) and (others == False):\n",
    "                    result.append(2)\n",
    "\n",
    "                else:\n",
    "                    if (others == True):\n",
    "                        result.append(0)\n",
    "                        \n",
    "                    else:\n",
    "                        result.append(4)\n",
    "                            \n",
    "    temporary_word_in_o = False\n",
    "    temporary_word_in_n = False\n",
    "    \n",
    "    for word in temporary_words:\n",
    "        if word in original_diff.lower():\n",
    "            temporary_word_in_o = True\n",
    "            \n",
    "        if word in new_diff.lower():\n",
    "            temporary_word_in_n = True\n",
    "                    \n",
    "    if (len(numbers_original) > 0) and (len(numbers_new) > 0):\n",
    "        if temporary_word_in_o == True and temporary_word_in_n == True:\n",
    "            result.append(1)\n",
    "            \n",
    "        if temporary_word_in_o == True and temporary_word_in_n == False:\n",
    "            result.append(2)\n",
    "            \n",
    "        if temporary_word_in_o == False and temporary_word_in_n == True:\n",
    "            result.append(3)\n",
    "            \n",
    "        if temporary_word_in_o == False and temporary_word_in_n == False:\n",
    "            result.append(4)\n",
    "            \n",
    "    else:\n",
    "        result.append(0)       \n",
    "            \n",
    "    \n",
    "    result.append(nlp(original_full).similarity(nlp(new_full)))\n",
    "    result.append(nlp(original_diff).similarity(nlp(new_diff)))\n",
    "    \n",
    "    doubt_words_o = 0\n",
    "    doubt_words_total = 0\n",
    "    \n",
    "    for token in doubt_words:\n",
    "        if token in original_diff.lower():\n",
    "            doubt_words_o = doubt_words_o + 1\n",
    "            doubt_words_total = doubt_words_total + 1\n",
    "            \n",
    "    for token in certain_words: \n",
    "        if token in original_diff.lower():\n",
    "            doubt_words_o = doubt_words_o - 1\n",
    "            doubt_words_total = doubt_words_total - 1\n",
    "    \n",
    "    if len(doc_diff_original) > 0:\n",
    "        result.append(abs(float(doubt_words_o)/len(doc_diff_original)))\n",
    "    else:\n",
    "        result.append(0)\n",
    "        \n",
    "    doubt_words_n = 0\n",
    "    for token in doubt_words:\n",
    "        if token in new_diff.lower():\n",
    "            doubt_words_n = doubt_words_n - 1\n",
    "            doubt_words_total = doubt_words_total - 1\n",
    "            \n",
    "    for token in certain_words: \n",
    "        if token in new_diff.lower():\n",
    "            doubt_words_n = doubt_words_n + 1\n",
    "            doubt_words_total = doubt_words_total + 1\n",
    "            \n",
    "    if len(doc_diff_new) > 0:\n",
    "        result.append(abs(float(doubt_words_n)/len(doc_diff_new)))\n",
    "    else:\n",
    "        result.append(0)\n",
    "    \n",
    "    #total doubt words\n",
    "    result.append(abs(doubt_words_total))\n",
    "\n",
    "    if other_ent_occurences_in_original == True:\n",
    "        result.append(1)\n",
    "        \n",
    "    else:\n",
    "        result.append(0)\n",
    "        \n",
    "    if other_ent_occurences_in_new == True:\n",
    "        result.append(1)\n",
    "        \n",
    "    else:\n",
    "        result.append(0)\n",
    "        \n",
    "    #look if word has changed its position or not    \n",
    "    if (new_token_sum - new_pos_counts['SPACE'] == 1) and (original_token_sum - original_pos_counts['SPACE'] == 1):\n",
    "        \n",
    "        if original_changed_sentences[0][2] == new_changed_sentences[0][2]:\n",
    "            result.append(2)\n",
    "            \n",
    "        else:\n",
    "            if (original_full[max(original_changed_sentences[0][2] - 4, 0): original_changed_sentences[0][2]] == new_full[max(new_changed_sentences[0][2] - 4, 0): new_changed_sentences[0][2]]) \\\n",
    "            and (original_full[original_changed_sentences[0][3] + 1: min(original_changed_sentences[0][3] + 4, len(original_full))] == new_full[new_changed_sentences[0][3] + 1: min(new_changed_sentences[0][3] + 4, len(new_full))]):\n",
    "                result.append(2)\n",
    "                \n",
    "            else:\n",
    "                result.append(1)\n",
    "                \n",
    "    else:\n",
    "        result.append(0)\n",
    "        \n",
    "    \n",
    "    #detailed entity change analysis\n",
    "    colors_o, days_o, curr_o, months_o, winds_o, states_o, countries_o, cities_o, belgian_o, nationality_o = entity_spotting(doc_diff_original)\n",
    "    colors_n, days_n, curr_n, months_n, winds_n, states_n, countries_n, cities_n, belgian_n, nationality_n = entity_spotting(doc_diff_new)\n",
    "    \n",
    "    #0 means no color data, 1 means no change between original and new, 2 means deletion, 3 means insertion, 4 means change into other color)\n",
    "    if (len(doc_diff_original) < 10) and (len(doc_diff_new) < 10): \n",
    "        result = array_comparison(colors_o, colors_n, result)\n",
    "        result = array_comparison(days_o, days_n, result)\n",
    "        result = array_comparison(curr_o, curr_n, result)\n",
    "        result = array_comparison(months_o, months_n, result)\n",
    "        result = array_comparison(winds_o, winds_n, result)\n",
    "        result = array_comparison(states_o, states_n, result)\n",
    "        result = array_comparison(countries_o, countries_n, result)\n",
    "        result = array_comparison(cities_o, cities_n, result)\n",
    "        result = array_comparison(belgian_o, belgian_n, result)\n",
    "        result = array_comparison(nationality_o, nationality_n, result)   \n",
    "        result = array_comparison(date_original, date_new, result)\n",
    "        result = array_comparison(person_original, person_new, result)\n",
    "        \n",
    "    else:\n",
    "        for i in range(0, 12):\n",
    "            result.append(6)\n",
    "    \n",
    "    negation_words_o = 0\n",
    "    for token in negation_words:\n",
    "        if token in negation_words:\n",
    "            negation_words_o = negation_words_o + 1\n",
    "\n",
    "    if len(doc_diff_original) > 0:\n",
    "        result.append(float(negation_words_o)/len(doc_diff_original))\n",
    "    else:\n",
    "        result.append(0)\n",
    "\n",
    "    negation_words_n = 0\n",
    "    for token in negation_words:\n",
    "        if token in new_diff.lower():\n",
    "            negation_words_n = negation_words_n + 1\n",
    "\n",
    "    if len(doc_diff_new) > 0:\n",
    "        result.append(float(negation_words_n)/len(doc_diff_new))\n",
    "    else:\n",
    "        result.append(0)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help function for the lemmatization of text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    arr = [w.lemma_ for w in doc]\n",
    "    return ' '.join(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yytimmer\\AppData\\Local\\Temp\\ipykernel_7892\\1669278784.py:801: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  result.append(nlp(original_full).similarity(nlp(new_full)))\n",
      "C:\\Users\\yytimmer\\AppData\\Local\\Temp\\ipykernel_7892\\1669278784.py:802: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  result.append(nlp(original_diff).similarity(nlp(new_diff)))\n"
     ]
    }
   ],
   "source": [
    "#code block calculating all features as described in the input data model dataset documentation\n",
    "#the code takes the raw data as an input and writes the manually constructed feature data to separate csv files\n",
    "\n",
    "#with open('..\\\\Data\\\\raw_data_input_obj.csv', newline='', encoding='utf-8') as csvfile:\n",
    "#with open('..\\\\Data\\\\raw_data_input_subj.csv', newline='', encoding='utf-8') as csvfile:\n",
    "with open('..\\\\Data\\\\raw_data_input_ling.csv', newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    \n",
    "    #new_file = open('..\\\\Data\\\\input_data_model_obj_manual.csv', 'w', newline='', encoding='utf-8')\n",
    "    #new_file = open('..\\\\Data\\\\input_data_model_subj_manual.csv', 'w', newline='', encoding='utf-8')\n",
    "    new_file = open('..\\\\Data\\\\input_data_model_ling_manual.csv', 'w', newline='', encoding='utf-8')\n",
    "    \n",
    "    writer = csv.writer(new_file, delimiter=',', quotechar='\"')\n",
    "    header = ['newspaper', 'topic', 'textpart', 'length_original', 'length_new', 'max_version_number', 'version_number_progress',\n",
    "                     'first_time', 'original_time', 'dates_difference', 'time_difference', 'original_title_length', 'original_intro_length',\n",
    "               'original_text_length', 'new_title_length', 'new_intro_length', 'new_text_length', 'new_time', \n",
    "              'fraction_original_title_changed', 'fraction_new_title_changed',\n",
    "              'fraction_original_intro_changed', 'fraction_new_intro_changed',\n",
    "              'fraction_original_text_changed', 'fraction_new_text_changed',\n",
    "              'fraction_total_changed_original', 'fraction_total_changed_new',\n",
    "              'fraction_total_changed_title_original', 'fraction_total_changed_title_new',\n",
    "              'fraction_total_changed_intro_original', 'fraction_total_changed_intro_new',\n",
    "              'fraction_total_changed_text_original', 'fraction_total_changed_text_new',\n",
    "              'type',\n",
    "                     'original_changed_fraction_text_part', 'new_changed_fraction_text_part', \n",
    "              'levenshtein_maximalized', 'nr_insert_max', 'nr_delete_max', 'nr_replace_max',\n",
    "              'levenshtein_minimalized', 'nr_insert_min', 'nr_delete_min', 'nr_replace_min', 'capitalized_equality',\n",
    "                     'jaccard', 'seqratio', 'text_overlap_original', 'text_overlap_new', \n",
    "              'original_lemmatized_minimized_changed_text', 'new_lemmatized_minimized_changed_text', 'stop_words_ratio',\n",
    "              'ent_original', 'original_token_length', 'adv_orig', 'noun_orig', 'point_orig', 'comma_orig',\n",
    "              'accent_orig', 'haakje_orig', 'doublepoint_orig', 'hyphen_orig', 'threepoints_orig', 'punct_orig', 'x_orig', 'propn_orig',\n",
    "              'pron_orig', 'det_orig', 'sconj_orig', 'space_orig', 'sym_orig', 'num_orig', 'adp_orig',\n",
    "              'intj_orig', 'aux_orig', 'inf_orig', 'pv_verl_ev_orig', 'pv_verl_mv_orig', 'pv_tgw_ev_orig', 'pv_tgw_mv_orig',\n",
    "                'od_prenom_orig', 'od_nom_orig', 'od_postnom_orig', 'od_vrij_orig', 'vd_vrij_orig', 'vd_prenom_orig', \n",
    "              'vd_postnom_orig', 'vd_nom_orig', 'verb_orig', 'cconj_orig', 'adj_sup_orig', 'adj_comp_orig', 'adj_basis_orig',\n",
    "              'ent_new', 'new_token_length', 'adv_new', 'noun_new', 'point_new', 'comma_new',\n",
    "              'accent_new', 'haakje_new', 'doublepoint_new', 'hyphen_new', 'threepoints_new', 'punct_new', 'x_new', 'propn_new',\n",
    "              'pron_new', 'det_new', 'sconj_new', 'space_new', 'sym_new', 'num_new', 'adp_new',\n",
    "              'intj_new', 'aux_new', 'inf_new', 'pv_verl_ev_new', 'pv_verl_mv_new', 'pv_tgw_ev_new', 'pv_tgw_mv_new',\n",
    "                'od_prenom_new', 'od_nom_new', 'od_postnom_new', 'od_vrij_new', 'vd_vrij_new', 'vd_prenom_new', \n",
    "              'vd_postnom_new', 'vd_nom_new', 'verb_new', 'cconj_new', 'adj_sup_new', 'adj_comp_new', 'adj_basis_new',\n",
    "              'double_word', 'equal_after_subst', 'globally_equal_after_subst',\n",
    "              'first_wordtype', 'last_wordtype', 'one_edit_change',\n",
    "               'orginal_spelling_ok', 'new_spelling_ok', 'number_comparison', 'temporary', 'sentence_sim', 'diff_sim',\n",
    "              'doubt_words_orig', 'doubt_words_new', 'doubt_words_total',\n",
    "              'entity_present_in_original', 'entity_present_in_new',\n",
    "              'changed_position',\n",
    "              'colors', 'days', 'currencies', 'months', 'winds', 'states', 'countries', 'cities', 'belgian', 'nationality',\n",
    "              'date_diff', 'person_diff',\n",
    "              'negation_original', 'negation_new',\n",
    "              'nr_red_parts', 'nr_green_parts', 'nr_full_sentences_original', 'nr_full_sentences_new',\n",
    "              'orig_part_of_new', 'new_part_of_orig', \n",
    "              'original_minimized_changed_text', 'new_minimized_changed_text',\n",
    "              'original_changed_text', 'new_changed_text'\n",
    "             ]\n",
    "        \n",
    "    writer.writerow(header)\n",
    "    \n",
    "    categorizationid = -1\n",
    "    original_article_text = []\n",
    "    new_article_text = []\n",
    "    original_changed_sentences = set()\n",
    "    new_changed_sentences = set()\n",
    "    new_row = []\n",
    "    in_other_text = False\n",
    "    \n",
    "    \n",
    "    #enumerate over all raw data records\n",
    "    for i, row in enumerate(reader):\n",
    "        if i != 0:\n",
    "            original = row[6]\n",
    "            \n",
    "            #if categorizationid of last row is not equal to that of the current row, we have reached the end of the previous atomic change\n",
    "            # because the raw data input files are sorted on categorizationid\n",
    "            #we can now calculate all features that require all atomic change information instead of only individual text part information\n",
    "            if (categorizationid != row[0]):\n",
    "                \n",
    "                if (categorizationid != -1):\n",
    "                    \n",
    "                    #original_changed_fraction_text_part\n",
    "                    if text_part == 'title':\n",
    "                        if len(original_title) > 0:\n",
    "                            new_row.append(new_row[3]/len(original_title))\n",
    "                        else:\n",
    "                            new_row.append(1.0)\n",
    "                    \n",
    "                    if text_part == 'intro':\n",
    "                        if len(original_intro) > 0:\n",
    "                            new_row.append(new_row[3]/len(original_intro))\n",
    "                        else:\n",
    "                            new_row.append(1.0)\n",
    "                        \n",
    "                    if text_part == 'text':\n",
    "                        if len(original_text) > 0:\n",
    "                            new_row.append(new_row[3]/len(original_text))\n",
    "                        else:\n",
    "                            new_row.append(1.0)\n",
    "                    \n",
    "                    #new_changed_fraction_text_part\n",
    "                    if text_part == 'title':\n",
    "                        if len(new_title) > 0:\n",
    "                            new_row.append(new_row[4]/len(new_title))\n",
    "                        else:\n",
    "                            new_row.append(1.0)\n",
    "                    \n",
    "                    if text_part == 'intro':\n",
    "                        if len(new_intro) > 0:\n",
    "                            new_row.append(new_row[4]/len(new_intro))\n",
    "                        else:\n",
    "                            new_row.append(1.0)\n",
    "                        \n",
    "                    if text_part == 'text':\n",
    "                        if len(new_text) > 0:\n",
    "                            new_row.append(new_row[4]/len(new_text))\n",
    "                        else:\n",
    "                            new_row.append(1.0)\n",
    "                    \n",
    "                    original_changed_sentences = sorted([(int(n1), int(n2), int(n3), int(n4)) for (n1, n2, n3, n4) in original_changed_sentences])\n",
    "                    new_changed_sentences = sorted([(int(n1), int(n2), int(n3), int(n4)) for (n1, n2, n3, n4) in new_changed_sentences])\n",
    "                    \n",
    "                    original_changed_text = \"\"\n",
    "                    new_changed_text = \"\"\n",
    "                    original_minimalized_changed_text = \"\"\n",
    "                    new_minimalized_changed_text = \"\"\n",
    "                    numeric = False\n",
    "                    nr_red_parts = 0\n",
    "                    nr_green_parts = 0\n",
    "                    previous_full = False\n",
    "                    \n",
    "                    nr_full_sentences_original = 0\n",
    "                    nr_full_sentences_new = 0\n",
    "                    original_sent_added = set()\n",
    "                    new_sent_added = set()\n",
    "                    \n",
    "                    #use collected unique identifiers of text parts belonging to the same atomic change in order\n",
    "                    #to construct the original full sentence, original minimalized sentence, new full sentence and new minimalized sentence\n",
    "                    \n",
    "                    #text parts that do not appear right after each other in the original text are separated by a single space\n",
    "                    for (i, (parnumber, sentencenumber, startindex, endindex)) in enumerate(original_changed_sentences):\n",
    "                        if i == 0:\n",
    "                            if not((parnumber, sentencenumber) in original_sent_added):\n",
    "                                original_changed_text = original_changed_text + original_article_text[int(parnumber)][int(sentencenumber)].strip()\n",
    "                                original_sent_added.add((parnumber, sentencenumber))\n",
    "                                \n",
    "                            original_minimalized_changed_text = original_minimalized_changed_text + original_article_text[int(parnumber)][int(sentencenumber)].strip()[int(startindex): int(endindex) + 1]\n",
    "                     \n",
    "                        else:\n",
    "                            if not((parnumber, sentencenumber) in original_sent_added):\n",
    "                                original_changed_text = original_changed_text + ' ' + original_article_text[int(parnumber)][int(sentencenumber)].strip()\n",
    "                                original_sent_added.add((parnumber, sentencenumber))\n",
    "                                \n",
    "                            original_minimalized_changed_text = original_minimalized_changed_text + ' ' + original_article_text[int(parnumber)][int(sentencenumber)].strip()[int(startindex): int(endindex) + 1]\n",
    "                    \n",
    "                        if (int(startindex) != 0) or (int(endindex) != len(original_article_text[int(parnumber)][int(sentencenumber)]) - 1):\n",
    "                            nr_red_parts = nr_red_parts + 1\n",
    "                            previous_full = False\n",
    "                        \n",
    "                        else:\n",
    "                            nr_full_sentences_original = nr_full_sentences_original + 1\n",
    "                            \n",
    "                            if not(previous_full):\n",
    "                                nr_red_parts = nr_red_parts + 1\n",
    "                                previous_full = True\n",
    "                    \n",
    "                    previous_full = False\n",
    "                    \n",
    "                    for (i, (parnumber, sentencenumber, startindex, endindex)) in enumerate(new_changed_sentences):\n",
    "                        if i == 0: \n",
    "                            if not((parnumber, sentencenumber) in new_sent_added):\n",
    "                                new_changed_text = new_changed_text + new_article_text[int(parnumber)][int(sentencenumber)].strip()\n",
    "                                new_sent_added.add((parnumber, sentencenumber))\n",
    "                            \n",
    "                            new_minimalized_changed_text = new_minimalized_changed_text + new_article_text[int(parnumber)][int(sentencenumber)].strip()[int(startindex): int(endindex) + 1]\n",
    "                                                \n",
    "                        else:\n",
    "                            if not((parnumber, sentencenumber) in new_sent_added):\n",
    "                                new_changed_text = new_changed_text + ' ' + new_article_text[int(parnumber)][int(sentencenumber)].strip()\n",
    "                                new_sent_added.add((parnumber, sentencenumber))\n",
    "                                \n",
    "                            new_minimalized_changed_text = new_minimalized_changed_text + ' ' + new_article_text[int(parnumber)][int(sentencenumber)].strip()[int(startindex): int(endindex) + 1]\n",
    "                    \n",
    "                        if (int(startindex) != 0) or (int(endindex) != len(new_article_text[int(parnumber)][int(sentencenumber)]) - 1):\n",
    "                            nr_green_parts = nr_green_parts + 1\n",
    "                            previous_full = False\n",
    "                        \n",
    "                        else:\n",
    "                            nr_full_sentences_new = nr_full_sentences_new + 1\n",
    "                            \n",
    "                            if not(previous_full):\n",
    "                                nr_green_parts = nr_green_parts + 1\n",
    "                                previous_full = True\n",
    "                                \n",
    "                    while '  ' in original_minimalized_changed_text:\n",
    "                        original_minimalized_changed_text = original_minimalized_changed_text.replace('  ', ' ')\n",
    "                        \n",
    "                    while '  ' in new_minimalized_changed_text:\n",
    "                        new_minimalized_changed_text = new_minimalized_changed_text.replace('  ', ' ')\n",
    "                    \n",
    "                    #add levenshtein distance\n",
    "                    levenshtein_distance_max = levenshtein_distance(original_changed_text.lower(), new_changed_text.lower())               \n",
    "                    \n",
    "                    new_row.append(levenshtein_distance_max)\n",
    "                    \n",
    "                    editops_list = editops(original_changed_text, new_changed_text)\n",
    "                    \n",
    "                    #add number of inserts\n",
    "                    new_row.append(len([op for (op, s, d) in editops_list if op == 'insert']))\n",
    "                    \n",
    "                    #add number of deletes\n",
    "                    new_row.append(len([op for (op, s, d) in editops_list if op == 'delete']))\n",
    "                    \n",
    "                    #add number of replacements\n",
    "                    new_row.append(len([op for (op, s, d) in editops_list if op == 'replace']))\n",
    "                    \n",
    "                    #add minimalized Levenshtein distance\n",
    "                    levenshtein_distance_min = levenshtein_distance(original_minimalized_changed_text.lower(), new_minimalized_changed_text.lower())\n",
    "                    \n",
    "                    new_row.append(levenshtein_distance_min)\n",
    "                    \n",
    "                    editops_list_minimalized = editops(original_minimalized_changed_text.lower(), new_minimalized_changed_text.lower())\n",
    "                    \n",
    "                    #add number of inserts\n",
    "                    new_row.append(len([op for (op, s, d) in editops_list_minimalized if op == 'insert']))\n",
    "                    \n",
    "                    #add number of deletes\n",
    "                    new_row.append(len([op for (op, s, d) in editops_list_minimalized if op == 'delete']))\n",
    "                    \n",
    "                    #add number of replacements\n",
    "                    new_row.append(len([op for (op, s, d) in editops_list_minimalized if op == 'replace']))\n",
    "                    \n",
    "                    \n",
    "                    #add capitalized_equality:\n",
    "                    if (original_minimalized_changed_text == new_minimalized_changed_text):\n",
    "                        new_row.append(0)\n",
    "                        \n",
    "                    else:\n",
    "                        if (original_minimalized_changed_text.lower() == new_minimalized_changed_text.lower()):\n",
    "                            new_row.append(1)\n",
    "                            \n",
    "                        else:\n",
    "                            if (original_minimalized_changed_text.strip() == new_minimalized_changed_text.strip()):\n",
    "                                new_row.append(2)\n",
    "                                \n",
    "                            else:\n",
    "                                str1 = original_minimalized_changed_text.lower().replace(' ', '').replace('-', '').replace('.','') \n",
    "                                str2 = new_minimalized_changed_text.lower().replace(' ', '').replace('-', '').replace('.','') \n",
    "                                \n",
    "                                if (str1 == str2):\n",
    "                                    new_row.append(3)\n",
    "                                    \n",
    "                                else:\n",
    "                                    new_row.append(4)\n",
    "                    \n",
    "                    #compute jaccard similarity\n",
    "                    set1 = set([string.lower() for string in word_tokenize(original_changed_text)])\n",
    "                    set2 = set([string.lower() for string in word_tokenize(new_changed_text)])\n",
    "                    \n",
    "                    jac_sim = float(len(set1.intersection(set2))) / len(set1.union(set2))\n",
    "\n",
    "                    \n",
    "                    new_row.append(jac_sim)\n",
    "                    \n",
    "                    new_row.append(seqratio([string for string in word_tokenize(original_changed_text)], [string for string in word_tokenize(new_changed_text)]))\n",
    "                    \n",
    "                    #add boolean whether at least one sentence is present in other text\n",
    "                    \n",
    "                    displaced_length_original = 0\n",
    "                    \n",
    "                    for (parnumber, sentencenumber, startindex, endindex) in original_changed_sentences:\n",
    "                        sentence = original_article_text[int(parnumber)][int(sentencenumber)]\n",
    "                        \n",
    "                        if ((sentence in new_title) or (sentence in new_intro) or (sentence in new_text)) and not(sentence in new_changed_text):\n",
    "                            displaced_length_original = displaced_length_original + len(sentence)\n",
    "                    \n",
    "                    displaced_length_new = 0        \n",
    "                            \n",
    "                    for (parnumber, sentencenumber, startindex, endindex) in new_changed_sentences:\n",
    "                        sentence = new_article_text[int(parnumber)][int(sentencenumber)]\n",
    "                            \n",
    "                        if ((sentence in original_title) or (sentence in original_intro) or (sentence in original_text)) and not (sentence in original_changed_text):\n",
    "                            displaced_length_new = displaced_length_new + len(sentence)      \n",
    "                                \n",
    "                    \n",
    "                    #new_row.append(in_other_text)\n",
    "                    if len(original_changed_text) == 0:\n",
    "                        new_row.append(0)\n",
    "                        \n",
    "                    else:\n",
    "                            \n",
    "                        new_row.append(float(displaced_length_original)/len(original_changed_text))\n",
    "                        \n",
    "                    if len(new_changed_text) == 0:\n",
    "                        new_row.append(0)   \n",
    "                        \n",
    "                    else:\n",
    "                        new_row.append(float(displaced_length_new)/len(new_changed_text))\n",
    "                    \n",
    "                    levenshtein_insert_string = ''\n",
    "                    levenshtein_delete_string = ''\n",
    "                    levenshtein_replace_string = ''\n",
    "                    \n",
    "                    insert_previous = None\n",
    "                    delete_previous = None\n",
    "                    replace_previous = None\n",
    "                    \n",
    "                    for (op, s, d) in editops_list:\n",
    "                        if op == 'insert':\n",
    "                            \n",
    "                            if(insert_previous != d - 1):\n",
    "                                levenshtein_insert_string = levenshtein_insert_string + ' ' + new_changed_text[d]\n",
    "                                \n",
    "                            else:\n",
    "                                levenshtein_insert_string = levenshtein_insert_string + new_changed_text[d]\n",
    "                                \n",
    "                            insert_previous = d\n",
    "                            \n",
    "                        if op == 'delete':\n",
    "                            if(delete_previous != s - 1):\n",
    "                                levenshtein_delete_string = levenshtein_delete_string + ' ' + original_changed_text[s]\n",
    "                                \n",
    "                            else:\n",
    "                                levenshtein_delete_string = levenshtein_delete_string + original_changed_text[s]\n",
    "                                \n",
    "                            delete_previous = s\n",
    "                            \n",
    "                        if op == 'replace':\n",
    "                            \n",
    "                            if(replace_previous != s - 1):\n",
    "                                levenshtein_replace_string = levenshtein_replace_string + ' ' + original_changed_text[s]\n",
    "                                \n",
    "                            else: \n",
    "                                levenshtein_replace_string = levenshtein_replace_string + original_changed_text[s]\n",
    "                                \n",
    "                            replace_previous = s\n",
    "                    \n",
    "                    \n",
    "                    #lemmatized minimized sentences\n",
    "                    new_row.append(lemmatize_text(original_minimalized_changed_text))\n",
    "                    new_row.append(lemmatize_text(new_minimalized_changed_text))\n",
    "                    \n",
    "                    verbose = False\n",
    "                    \n",
    "                    #spaCy features are added\n",
    "                        \n",
    "                    if text_part == 'title':\n",
    "                    \n",
    "                        additional_features = use_spacy(verbose, original_changed_text, original_minimalized_changed_text, \n",
    "                                                    new_changed_text, new_minimalized_changed_text, original_title, new_title, \\\n",
    "                                                        original_changed_sentences, new_changed_sentences)\n",
    "                    \n",
    "                    if text_part == 'intro':\n",
    "                    \n",
    "                        additional_features = use_spacy(verbose, original_changed_text, original_minimalized_changed_text, \n",
    "                                                    new_changed_text, new_minimalized_changed_text, original_intro, new_intro, \\\n",
    "                                                        original_changed_sentences, new_changed_sentences)\n",
    "                                    \n",
    "                    if text_part == 'text':\n",
    "                    \n",
    "                        additional_features = use_spacy(verbose, original_changed_text, original_minimalized_changed_text, \n",
    "                                                    new_changed_text, new_minimalized_changed_text, original_text, new_text, \\\n",
    "                                                        original_changed_sentences, new_changed_sentences)\n",
    "                    \n",
    "                    for feature in additional_features:\n",
    "                        new_row.append(feature)\n",
    "                        \n",
    "                        \n",
    "                    #number_red_parts\n",
    "                    new_row.append(nr_red_parts)\n",
    "                    \n",
    "                    #number_green_parts\n",
    "                    new_row.append(nr_green_parts)\n",
    "                    \n",
    "                    #number of original full sentences\n",
    "                    new_row.append(nr_full_sentences_original)\n",
    "                    \n",
    "                    #number of new full sentences\n",
    "                    new_row.append(nr_full_sentences_new)\n",
    "                    \n",
    "                    \n",
    "                    #original subpart of new\n",
    "                    if original_minimalized_changed_text.strip() in new_minimalized_changed_text.strip() and len(original_minimalized_changed_text.strip()) > 1:\n",
    "                        new_row.append(1)\n",
    "                        \n",
    "                    else:\n",
    "                        new_row.append(0)\n",
    "                    \n",
    "                    #new subpart of original\n",
    "                    if new_minimalized_changed_text.strip() in original_minimalized_changed_text.strip() and len(new_minimalized_changed_text.strip()) > 1:\n",
    "                        new_row.append(1)\n",
    "                    else:\n",
    "                        new_row.append(0)\n",
    "                    \n",
    "                    #minimized_changed_text\n",
    "                    new_row.append(original_minimalized_changed_text)\n",
    "                    new_row.append(new_minimalized_changed_text)\n",
    "                    \n",
    "                    #changed_text \n",
    "                    new_row.append(original_changed_text)\n",
    "                    new_row.append(new_changed_text)  \n",
    "                \n",
    "                    writer.writerow(new_row)\n",
    "                    new_row = []\n",
    "                    original_changed_sentences = set()\n",
    "                    new_changed_sentences = set()\n",
    "                    \n",
    "                 \n",
    "                text_part = row[5]\n",
    "                \n",
    "                while u'\\xa0\\xa0' in row[11]:\n",
    "                    row[11] = row[11].replace(u'\\xa0\\xa0', u'\\xa0')\n",
    "                    \n",
    "                while u'\\xa0\\xa0' in row[12]:\n",
    "                    row[12] = row[12].replace(u'\\xa0\\xa0', u'\\xa0')\n",
    "                    \n",
    "                while u'\\xa0\\xa0' in row[13]:\n",
    "                    row[13] = row[13].replace(u'\\xa0\\xa0', u'\\xa0')\n",
    "                    \n",
    "                while u'\\xa0\\xa0' in row[14]:\n",
    "                    row[14] = row[14].replace(u'\\xa0\\xa0', u'\\xa0')\n",
    "                    \n",
    "                while u'\\xa0\\xa0' in row[15]:\n",
    "                    row[15] = row[15].replace(u'\\xa0\\xa0', u'\\xa0')\n",
    "                    \n",
    "                while u'\\xa0\\xa0' in row[16]:\n",
    "                    row[16] = row[16].replace(u'\\xa0\\xa0', u'\\xa0')\n",
    "                \n",
    "                original_title = row[11]\n",
    "                new_title = row[12]\n",
    "                original_intro = row[13]\n",
    "                new_intro = row[14]\n",
    "                original_text = row[15]\n",
    "                new_text = row[16]\n",
    "                    \n",
    "                if text_part == 'title':\n",
    "                    original_article_text = split_text(row[11])\n",
    "                    new_article_text = split_text(row[12])\n",
    "\n",
    "                if text_part == 'intro':\n",
    "                    original_article_text = split_text(row[13])\n",
    "                    new_article_text = split_text(row[14])\n",
    "\n",
    "                if text_part == 'text':\n",
    "                    original_article_text = split_text(row[15])\n",
    "                    new_article_text = split_text(row[16])\n",
    "                    \n",
    "                in_other_text = False\n",
    "                    \n",
    "                categorizationid = row[0]\n",
    "                original_article = []\n",
    "                new_article = []\n",
    "                                \n",
    "                new_row.append(row[3]) #newspaper\n",
    "                new_row.append(row[4]) #topic\n",
    "                new_row.append(text_part) #textpart\n",
    "                \n",
    "                #original length\n",
    "                if original == 'true':\n",
    "                    new_row.append(int(row[10]) - int(row[9]) + 1)\n",
    "                    new_row.append(0)\n",
    "                    \n",
    "                #new length\n",
    "                else:\n",
    "                    new_row.append(0)\n",
    "                    new_row.append(int(row[10]) - int(row[9]) + 1)\n",
    "                    \n",
    "                #max_version_number\n",
    "                new_row.append(row[30])\n",
    "                \n",
    "                #version_number_progress\n",
    "                if original == 'true':\n",
    "                    new_row.append((float(row[2]) + 1)/float(row[30]))\n",
    "                    \n",
    "                else: \n",
    "                    new_row.append(float(row[2])/float(row[30]))\n",
    "                \n",
    "                #first_time\n",
    "                new_row.append(row[19])\n",
    "                \n",
    "                #original_time\n",
    "                new_row.append(row[17])\n",
    "                \n",
    "                #check if publication date of both versions is the same\n",
    "                original_date = datetime.strptime(row[17], '%Y-%m-%d %H:%M:%S.%f')\n",
    "                new_date = datetime.strptime(row[18], '%Y-%m-%d %H:%M:%S.%f')\n",
    "                \n",
    "                if original_date.date() == new_date.date():\n",
    "                    new_row.append(1)\n",
    "                else:\n",
    "                    new_row.append(0)\n",
    "                                \n",
    "                #time_difference\n",
    "                new_row.append(row[32])\n",
    "            \n",
    "                #original_title_length\n",
    "                new_row.append(len(row[11]))\n",
    "                \n",
    "                #original_intro_length\n",
    "                new_row.append(len(row[13]))\n",
    "                \n",
    "                #original_text_length\n",
    "                new_row.append(len(row[15]))\n",
    "                \n",
    "                #original_title_length\n",
    "                new_row.append(len(row[12]))\n",
    "                \n",
    "                #original_intro_length\n",
    "                new_row.append(len(row[14]))\n",
    "                \n",
    "                #original_text_length\n",
    "                new_row.append(len(row[16]))\n",
    "                \n",
    "                #new_time\n",
    "                new_row.append(row[18])\n",
    "                \n",
    "                #store parts of atomic change by their unique indexes (parnumber, sentencenumber, startindex and endindex)\n",
    "                if original == 'true':\n",
    "                    original_changed_sentences.add((row[7], row[8], row[9], row[10]))\n",
    "                else:\n",
    "                    new_changed_sentences.add((row[7], row[8], row[9], row[10]))\n",
    "                    \n",
    "                #add changed fractions for different article parts\n",
    "                if (len(original_title) == 0):\n",
    "                    new_row.append(1.0)\n",
    "                else:\n",
    "                    new_row.append(float(row[34])/len(original_title))\n",
    "                    \n",
    "                if (len(new_title) == 0):\n",
    "                    new_row.append(1.0)\n",
    "                else:\n",
    "                    new_row.append(float(row[35])/len(new_title))\n",
    "                    \n",
    "                if (len(original_intro) == 0):\n",
    "                    new_row.append(1.0)\n",
    "                else:\n",
    "                    new_row.append(float(row[36])/len(original_intro))\n",
    "                    \n",
    "                if (len(new_intro) == 0):\n",
    "                    new_row.append(1.0)\n",
    "                else:\n",
    "                    new_row.append(float(row[37])/len(new_intro))\n",
    "                    \n",
    "                if (len(original_text) == 0):\n",
    "                    new_row.append(1.0)\n",
    "                else:\n",
    "                    new_row.append(float(row[38])/len(original_text))\n",
    "                    \n",
    "                if (len(new_text) == 0):\n",
    "                    new_row.append(1.0)\n",
    "                else:\n",
    "                    new_row.append(float(row[39])/len(new_text))\n",
    "                \n",
    "                #add total changed fractions to text\n",
    "                new_row.append(row[20])\n",
    "                new_row.append(row[21])\n",
    "                new_row.append(row[22])\n",
    "                new_row.append(row[23])\n",
    "                new_row.append(row[24])\n",
    "                new_row.append(row[25])\n",
    "                new_row.append(row[26])\n",
    "                new_row.append(row[27])\n",
    "                \n",
    "                #label (an error or not?)\n",
    "                new_row.append(row[29])\n",
    "                \n",
    "            else:\n",
    "                #original length\n",
    "                if original == 'true':\n",
    "                    new_row[3] = new_row[3] + int(row[10]) - int(row[9]) + 1\n",
    "                    \n",
    "                #new length\n",
    "                else:\n",
    "                    new_row[4] = new_row[4] +  int(row[10]) - int(row[9]) + 1\n",
    "                    \n",
    "                    \n",
    "                #store parts of atomic change by their unique indexes (parnumber, sentencenumber, startindex and endindex)\n",
    "                if original == 'true':\n",
    "                    original_changed_sentences.add((row[7], row[8], row[9], row[10]))\n",
    "                else:\n",
    "                    new_changed_sentences.add((row[7], row[8], row[9], row[10]))\n",
    "                    \n",
    "    new_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at GroNLP/bert-base-dutch-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "model = AutoModel.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code block that takes manual features as an input and that calculates BERTje embeddings on full textual data\n",
    "#these embeddings are subsequently written to a separate file\n",
    "\n",
    "#this code block should only be ran once! (embeddings are the same for objective, subjective and linguistic errors)\n",
    "\n",
    "bertje_file = open(\"..\\\\Data\\\\bertje.csv\", 'w+', newline='')\n",
    "inputfile = open(\"..\\\\Data\\\\input_data_model_ling_manual.csv\", encoding='utf-8')\n",
    "\n",
    "objbertjewriter = csv.writer(bertje_file, delimiter=';')\n",
    "reader = csv.reader(inputfile, delimiter=',')\n",
    "\n",
    "for i, row in enumerate(reader):\n",
    "    \n",
    "    if i != 0:\n",
    "        original_changed_text = row[-2]\n",
    "        new_changed_text = row[-1]\n",
    "\n",
    "        original_df = tokenizer(original_changed_text, padding = True, truncation = True, return_tensors=\"pt\")\n",
    "        new_df = tokenizer(new_changed_text, padding = True, truncation = True, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            hidden_original = model(**original_df)\n",
    "            hidden_new = model(**new_df)\n",
    "\n",
    "            #get only the [CLS] hidden states\n",
    "            cls = hidden_original.last_hidden_state[:,0,:].tolist()[0]\n",
    "            cls_new = hidden_new.last_hidden_state[:,0,:].tolist()[0]\n",
    "            cls.extend(cls_new)\n",
    "\n",
    "            objbertjewriter.writerow(cls)\n",
    "\n",
    "bertje_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code block that takes manual features as an input and that calculates BERTje embeddings on minimized (but not lemmatized) textual data\n",
    "#these embeddings are subsequently written to a separate file\n",
    "\n",
    "#this code block should only be ran once! (embeddings are the same for objective, subjective and linguistic errors)\n",
    "\n",
    "bertje_file = open(\"..\\\\Data\\\\bertje_minimized.csv\", 'w+', newline='')\n",
    "inputfile = open(\"..\\\\Data\\\\input_data_model_ling_manual.csv\", encoding='utf-8')\n",
    "\n",
    "objbertjewriter = csv.writer(bertje_file, delimiter=';')\n",
    "reader = csv.reader(inputfile, delimiter=',')\n",
    "\n",
    "for i, row in enumerate(reader):\n",
    "    \n",
    "    if i != 0:\n",
    "        original_changed_text = row[-4]\n",
    "        new_changed_text = row[-3]\n",
    "\n",
    "        original_df = tokenizer(original_changed_text, padding = True, truncation = True, return_tensors=\"pt\")\n",
    "        new_df = tokenizer(new_changed_text, padding = True, truncation = True, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            hidden_original = model(**original_df)\n",
    "            hidden_new = model(**new_df)\n",
    "\n",
    "            #get only the [CLS] hidden states\n",
    "            cls = hidden_original.last_hidden_state[:,0,:].tolist()[0]\n",
    "            cls_new = hidden_new.last_hidden_state[:,0,:].tolist()[0]\n",
    "            cls.extend(cls_new)\n",
    "\n",
    "            objbertjewriter.writerow(cls)\n",
    "    \n",
    "bertje_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code block that takes manual features as an input and that calculates BERTje embeddings on lemmatized and minimized textual data\n",
    "#these embeddings are subsequently written to a separate file\n",
    "\n",
    "#this code block should only be ran once! (embeddings are the same for objective, subjective and linguistic errors)\n",
    "\n",
    "bertje_file = open(\"..\\\\Data\\\\bertje_lemmatized.csv\", 'w+', newline='')\n",
    "inputfile = open(\"..\\\\Data\\\\input_data_model_ling_manual.csv\", encoding='utf-8')\n",
    "\n",
    "objbertjewriter = csv.writer(bertje_file, delimiter=';')\n",
    "reader = csv.reader(inputfile, delimiter=',')\n",
    "\n",
    "for i, row in enumerate(reader):\n",
    "    \n",
    "    if i != 0:\n",
    "        original_changed_text = row[48]\n",
    "        new_changed_text = row[49]\n",
    "\n",
    "        original_df = tokenizer(original_changed_text, padding = True, truncation = True, return_tensors=\"pt\")\n",
    "        new_df = tokenizer(new_changed_text, padding = True, truncation = True, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            hidden_original = model(**original_df)\n",
    "            hidden_new = model(**new_df)\n",
    "\n",
    "            #get only the [CLS] hidden states\n",
    "            cls = hidden_original.last_hidden_state[:,0,:].tolist()[0]\n",
    "            cls_new = hidden_new.last_hidden_state[:,0,:].tolist()[0]\n",
    "            cls.extend(cls_new)\n",
    "\n",
    "            objbertjewriter.writerow(cls)\n",
    "    \n",
    "bertje_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "sbert_model.max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code block that takes manual features as an input and that calculates SBERT embeddings on full textual data\n",
    "#these embeddings are subsequently written to a separate file\n",
    "\n",
    "#this code block should only be ran once! (embeddings are the same for objective, subjective and linguistic errors)\n",
    "\n",
    "sbert_file = open(\"..\\\\Data\\\\sbert.csv\", 'w+', newline='')\n",
    "inputfile = open(\"..\\\\Data\\\\input_data_model_ling_manual.csv\", encoding='utf-8')\n",
    "\n",
    "objsbertwriter = csv.writer(sbert_file, delimiter=';')\n",
    "reader = csv.reader(inputfile, delimiter=',')\n",
    "\n",
    "for i, row in enumerate(reader):\n",
    "    \n",
    "    if i != 0:\n",
    "        original_changed_text = row[-2]\n",
    "        new_changed_text = row[-1]\n",
    "\n",
    "        original_embedding = sbert_model.encode(original_changed_text).tolist()\n",
    "        new_embedding = sbert_model.encode(new_changed_text).tolist()\n",
    "        \n",
    "        original_embedding.extend(new_embedding)\n",
    "\n",
    "        objsbertwriter.writerow(original_embedding)\n",
    "\n",
    "    \n",
    "sbert_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code block that takes manual features as an input and that calculates SBERT embeddings on minimized textual data\n",
    "#these embeddings are subsequently written to a separate file\n",
    "\n",
    "#this code block should only be ran once! (embeddings are the same for objective, subjective and linguistic errors)\n",
    "\n",
    "sbert_file = open(\"..\\\\Data\\\\sbert_minimized.csv\", 'w+', newline='')\n",
    "inputfile = open(\"..\\\\Data\\\\input_data_model_ling_manual.csv\", encoding='utf-8')\n",
    "\n",
    "objsbertwriter = csv.writer(sbert_file, delimiter=';')\n",
    "reader = csv.reader(inputfile, delimiter=',')\n",
    "\n",
    "for i, row in enumerate(reader):\n",
    "    \n",
    "    if i != 0:\n",
    "        original_changed_text = row[-4]\n",
    "        new_changed_text = row[-3]\n",
    "\n",
    "        original_embedding = sbert_model.encode(original_changed_text).tolist()\n",
    "        new_embedding = sbert_model.encode(new_changed_text).tolist()\n",
    "        \n",
    "        original_embedding.extend(new_embedding)\n",
    "\n",
    "        objsbertwriter.writerow(original_embedding)\n",
    "    \n",
    "sbert_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code block that takes manual features as an input and that calculates SBERT embeddings on minimized and lemmatized textual data\n",
    "#these embeddings are subsequently written to a separate file\n",
    "\n",
    "#this code block should only be ran once! (embeddings are the same for objective, subjective and linguistic errors)\n",
    "\n",
    "sbert_file = open(\"..\\\\Data\\\\sbert_lemmatized.csv\", 'w+', newline='')\n",
    "inputfile = open(\"..\\\\Data\\\\input_data_model_ling_manual.csv\", encoding='utf-8')\n",
    "\n",
    "objsbertwriter = csv.writer(sbert_file, delimiter=';')\n",
    "reader = csv.reader(inputfile, delimiter=',')\n",
    "\n",
    "for i, row in enumerate(reader):\n",
    "    \n",
    "    if i != 0:\n",
    "        original_changed_text = row[48]\n",
    "        new_changed_text = row[49]\n",
    "\n",
    "        original_embedding = sbert_model.encode(original_changed_text).tolist()\n",
    "        new_embedding = sbert_model.encode(new_changed_text).tolist()\n",
    "        \n",
    "        original_embedding.extend(new_embedding)\n",
    "\n",
    "        objsbertwriter.writerow(original_embedding)\n",
    "    \n",
    "sbert_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "model = AutoModel.from_pretrained(\"GroNLP/bert-base-dutch-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all embeddings and add them all in one single row per sample in the dataset\n",
    "\n",
    "bertje_input_file = open(\"..\\\\Data\\\\bertje.csv\", encoding='utf-8')\n",
    "\n",
    "bertje_reader = csv.reader(bertje_input_file, delimiter=';')\n",
    "\n",
    "embeddings = []\n",
    "for row in bertje_reader:\n",
    "    embeddings.append(row)\n",
    "    \n",
    "bertje_input_file.close()\n",
    "\n",
    "#read BERTje lemmatized embeddings\n",
    "\n",
    "bertje_input_file = open(\"..\\\\Data\\\\bertje_lemmatized.csv\", encoding='utf-8')\n",
    "\n",
    "bertje_reader = csv.reader(bertje_input_file, delimiter=';')\n",
    "\n",
    "for i, row in enumerate(bertje_reader):\n",
    "    embeddings[i].extend(row)\n",
    "    \n",
    "bertje_input_file.close()\n",
    "\n",
    "#read BERTje minimized embeddings\n",
    "\n",
    "bertje_input_file = open(\"..\\\\Data\\\\bertje_minimized.csv\", encoding='utf-8')\n",
    "\n",
    "bertje_reader = csv.reader(bertje_input_file, delimiter=';')\n",
    "\n",
    "for i, row in enumerate(bertje_reader):\n",
    "    embeddings[i].extend(row)\n",
    "    \n",
    "bertje_input_file.close()\n",
    "\n",
    "#read SBERT embeddings\n",
    "\n",
    "sbert_input_file = open(\"..\\\\Data\\\\sbert.csv\", encoding='utf-8')\n",
    "\n",
    "sbert_reader = csv.reader(sbert_input_file, delimiter=';')\n",
    "\n",
    "for i, row in enumerate(sbert_reader):\n",
    "    embeddings[i].extend(row)\n",
    "    \n",
    "sbert_input_file.close()\n",
    "\n",
    "#read SBERT lemmatized embeddings\n",
    "\n",
    "sbert_input_file = open(\"..\\\\Data\\\\sbert_lemmatized.csv\", encoding='utf-8')\n",
    "\n",
    "sbert_reader = csv.reader(sbert_input_file, delimiter=';')\n",
    "\n",
    "for i, row in enumerate(sbert_reader):\n",
    "    embeddings[i].extend(row)\n",
    "    \n",
    "sbert_input_file.close()\n",
    "\n",
    "#read SBERT minimized embeddings\n",
    "\n",
    "sbert_input_file = open(\"..\\\\Data\\\\sbert_minimized.csv\", encoding='utf-8')\n",
    "\n",
    "sbert_reader = csv.reader(sbert_input_file, delimiter=';')\n",
    "\n",
    "for i, row in enumerate(sbert_reader):\n",
    "    embeddings[i].extend(row)\n",
    "    \n",
    "sbert_input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code block that adds all embeddings to the feature data set for the objective error task\n",
    "\n",
    "obj_input_file = open(\"..\\\\Data\\\\input_data_model_obj_manual.csv\", encoding='utf-8')\n",
    "obj_output_file = open(\"..\\\\Data\\\\input_data_model_obj.csv\", 'w+', newline='', encoding='utf-8')\n",
    "\n",
    "header_bertje_original = ['original_bertje_' + str(i) for i in range(0, 768)]\n",
    "header_bertje_new = ['new_bertje_' + str(i) for i in range(0, 768)]\n",
    "\n",
    "header_bertje_lemmatized_original = ['original_lemmatized_bertje_' + str(i) for i in range(0, 768)]\n",
    "header_bertje_lemmatized_new = ['new_lemmatized_bertje_' + str(i) for i in range(0, 768)]\n",
    "\n",
    "header_bertje_minimized_original = ['original_minimized_bertje_' + str(i) for i in range(0, 768)]\n",
    "header_bertje_minimized_new = ['new_minimized_bertje_' + str(i) for i in range(0, 768)]\n",
    "\n",
    "header_sbert_original = ['original_sbert_' + str(i) for i in range(0, 512)]\n",
    "header_sbert_new = ['new_sbert_' + str(i) for i in range(0, 512)]\n",
    "\n",
    "header_sbert_lemmatized_original = ['original_lemmatized_sbert_' + str(i) for i in range(0, 512)]\n",
    "header_sbert_lemmatized_new = ['new_lemmatized_sbert_' + str(i) for i in range(0, 512)]\n",
    "\n",
    "header_sbert_minimized_original = ['original_minimized_sbert_' + str(i) for i in range(0, 512)]\n",
    "header_sbert_minimized_new = ['new_minimized_sbert_' + str(i) for i in range(0, 512)]\n",
    "\n",
    "writer = csv.writer(obj_output_file, delimiter=',')\n",
    "reader = csv.reader(obj_input_file, delimiter=',')\n",
    "\n",
    "for i, row in enumerate(reader):\n",
    "    if i == 0:\n",
    "        row.extend(header_bertje_original)\n",
    "        row.extend(header_bertje_new)\n",
    "        row.extend(header_bertje_lemmatized_original)\n",
    "        row.extend(header_bertje_lemmatized_new)\n",
    "        row.extend(header_bertje_minimized_original)\n",
    "        row.extend(header_bertje_minimized_new)\n",
    "        \n",
    "        row.extend(header_sbert_original)\n",
    "        row.extend(header_sbert_new)\n",
    "        row.extend(header_sbert_lemmatized_original)\n",
    "        row.extend(header_sbert_lemmatized_new)\n",
    "        row.extend(header_sbert_minimized_original)\n",
    "        row.extend(header_sbert_minimized_new)\n",
    "        \n",
    "        writer.writerow(row)\n",
    "    \n",
    "    if i != 0:\n",
    "        row.extend(embeddings[i-1])\n",
    "        writer.writerow(row)\n",
    "    \n",
    "obj_input_file.close()\n",
    "obj_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code block that adds all embeddings to the feature data set for the subjective error task\n",
    "\n",
    "subj_input_file = open(\"..\\\\Data\\\\input_data_model_subj_manual.csv\", encoding='utf-8')\n",
    "subj_output_file = open(\"..\\\\Data\\\\input_data_model_subj.csv\", 'w+', newline='', encoding='utf-8')\n",
    "\n",
    "header_bertje_original = ['original_bertje_' + str(i) for i in range(0, 768)]\n",
    "header_bertje_new = ['new_bertje_' + str(i) for i in range(0, 768)]\n",
    "\n",
    "header_bertje_lemmatized_original = ['original_lemmatized_bertje_' + str(i) for i in range(0, 768)]\n",
    "header_bertje_lemmatized_new = ['new_lemmatized_bertje_' + str(i) for i in range(0, 768)]\n",
    "\n",
    "header_bertje_minimized_original = ['original_minimized_bertje_' + str(i) for i in range(0, 768)]\n",
    "header_bertje_minimized_new = ['new_minimized_bertje_' + str(i) for i in range(0, 768)]\n",
    "\n",
    "header_sbert_original = ['original_sbert_' + str(i) for i in range(0, 512)]\n",
    "header_sbert_new = ['new_sbert_' + str(i) for i in range(0, 512)]\n",
    "\n",
    "header_sbert_lemmatized_original = ['original_lemmatized_sbert_' + str(i) for i in range(0, 512)]\n",
    "header_sbert_lemmatized_new = ['new_lemmatized_sbert_' + str(i) for i in range(0, 512)]\n",
    "\n",
    "header_sbert_minimized_original = ['original_minimized_sbert_' + str(i) for i in range(0, 512)]\n",
    "header_sbert_minimized_new = ['new_minimized_sbert_' + str(i) for i in range(0, 512)]\n",
    "\n",
    "writer = csv.writer(subj_output_file, delimiter=',')\n",
    "reader = csv.reader(subj_input_file, delimiter=',')\n",
    "\n",
    "for i, row in enumerate(reader):\n",
    "    if i == 0:\n",
    "        row.extend(header_bertje_original)\n",
    "        row.extend(header_bertje_new)\n",
    "        row.extend(header_bertje_lemmatized_original)\n",
    "        row.extend(header_bertje_lemmatized_new)\n",
    "        row.extend(header_bertje_minimized_original)\n",
    "        row.extend(header_bertje_minimized_new)\n",
    "        \n",
    "        row.extend(header_sbert_original)\n",
    "        row.extend(header_sbert_new)\n",
    "        row.extend(header_sbert_lemmatized_original)\n",
    "        row.extend(header_sbert_lemmatized_new)\n",
    "        row.extend(header_sbert_minimized_original)\n",
    "        row.extend(header_sbert_minimized_new)\n",
    "        writer.writerow(row)\n",
    "    \n",
    "    if i != 0:\n",
    "        row.extend(embeddings[i-1])\n",
    "        writer.writerow(row)\n",
    "    \n",
    "subj_input_file.close()\n",
    "subj_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code block that adds all embeddings to the feature data set for the linguistic error task\n",
    "\n",
    "spel_input_file = open(\"..\\\\Data\\\\input_data_model_ling_manual.csv\", encoding='utf-8')\n",
    "spel_output_file = open(\"..\\\\Data\\\\input_data_model_ling.csv\", 'w+', newline='', encoding='utf-8')\n",
    "\n",
    "header_bertje_original = ['original_bertje_' + str(i) for i in range(0, 768)]\n",
    "header_bertje_new = ['new_bertje_' + str(i) for i in range(0, 768)]\n",
    "\n",
    "header_bertje_lemmatized_original = ['original_lemmatized_bertje_' + str(i) for i in range(0, 768)]\n",
    "header_bertje_lemmatized_new = ['new_lemmatized_bertje_' + str(i) for i in range(0, 768)]\n",
    "\n",
    "header_bertje_minimized_original = ['original_minimized_bertje_' + str(i) for i in range(0, 768)]\n",
    "header_bertje_minimized_new = ['new_minimized_bertje_' + str(i) for i in range(0, 768)]\n",
    "\n",
    "header_sbert_original = ['original_sbert_' + str(i) for i in range(0, 512)]\n",
    "header_sbert_new = ['new_sbert_' + str(i) for i in range(0, 512)]\n",
    "\n",
    "header_sbert_lemmatized_original = ['original_lemmatized_sbert_' + str(i) for i in range(0, 512)]\n",
    "header_sbert_lemmatized_new = ['new_lemmatized_sbert_' + str(i) for i in range(0, 512)]\n",
    "\n",
    "header_sbert_minimized_original = ['original_minimized_sbert_' + str(i) for i in range(0, 512)]\n",
    "header_sbert_minimized_new = ['new_minimized_sbert_' + str(i) for i in range(0, 512)]\n",
    "\n",
    "writer = csv.writer(spel_output_file, delimiter=',')\n",
    "reader = csv.reader(spel_input_file, delimiter=',')\n",
    "\n",
    "for i, row in enumerate(reader):\n",
    "    if i == 0:\n",
    "        row.extend(header_bertje_original)\n",
    "        row.extend(header_bertje_new)\n",
    "        row.extend(header_bertje_lemmatized_original)\n",
    "        row.extend(header_bertje_lemmatized_new)\n",
    "        row.extend(header_bertje_minimized_original)\n",
    "        row.extend(header_bertje_minimized_new)\n",
    "        \n",
    "        row.extend(header_sbert_original)\n",
    "        row.extend(header_sbert_new)\n",
    "        row.extend(header_sbert_lemmatized_original)\n",
    "        row.extend(header_sbert_lemmatized_new)\n",
    "        row.extend(header_sbert_minimized_original)\n",
    "        row.extend(header_sbert_minimized_new)\n",
    "        writer.writerow(row)\n",
    "    \n",
    "    if i != 0:\n",
    "        row.extend(embeddings[i-1])\n",
    "        writer.writerow(row)\n",
    "    \n",
    "spel_input_file.close()\n",
    "spel_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code block that takes the feature file as an input (which can be constructed using the code blocks above)\n",
    "#and outputs two separate files, one containing the training set, one containing the test set\n",
    "#records are split randomly, but such that stratified k-fold cross validation is possible \n",
    "#(equal relative amount of errors in training set and test set)\n",
    "\n",
    "#new_test_file = open('..\\\\Data\\\\input_data_model_obj_test.csv', 'w', newline='', encoding='utf-8')\n",
    "#new_train_file = open('..\\\\Data\\\\input_data_model_obj_train.csv', 'w', newline='', encoding='utf-8')\n",
    "#new_test_file = open('..\\\\Data\\\\input_data_model_subj_test.csv', 'w', newline='', encoding='utf-8')\n",
    "#new_train_file = open('..\\\\Data\\\\input_data_model_subj_train.csv', 'w', newline='', encoding='utf-8')\n",
    "new_test_file = open('..\\\\Data\\\\input_data_model_ling_test.csv', 'w', newline='', encoding='utf-8')\n",
    "new_train_file = open('..\\\\Data\\\\input_data_model_ling_train.csv', 'w', newline='', encoding='utf-8')\n",
    "\n",
    "\n",
    "#csvfile = open('..\\\\Data\\\\input_data_model_obj.csv', 'r', newline='', encoding='utf-8')\n",
    "#csvfile = open('..\\\\Data\\\\input_data_model_subj.csv', 'r', newline='', encoding='utf-8')\n",
    "csvfile = open('..\\\\Data\\\\input_data_model_ling.csv', 'r', newline='', encoding='utf-8')\n",
    "\n",
    "reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "\n",
    "test_writer = csv.writer(new_test_file, delimiter=',', quotechar='\"')\n",
    "header = ['newspaper', 'topic', 'textpart', 'length_original', 'length_new', 'max_version_number', 'version_number_progress',\n",
    "                     'first_time', 'original_time', 'dates_difference', 'time_difference', 'original_title_length', 'original_intro_length',\n",
    "               'original_text_length', 'new_title_length', 'new_intro_length', 'new_text_length', 'new_time', \n",
    "              'fraction_original_title_changed', 'fraction_new_title_changed',\n",
    "              'fraction_original_intro_changed', 'fraction_new_intro_changed',\n",
    "              'fraction_original_text_changed', 'fraction_new_text_changed',\n",
    "          'fraction_total_changed_original', 'fraction_total_changed_new',\n",
    "            'fraction_total_changed_title_original', 'fraction_total_changed_title_new',\n",
    "              'fraction_total_changed_intro_original', 'fraction_total_changed_intro_new',\n",
    "              'fraction_total_changed_text_original', 'fraction_total_changed_text_new',\n",
    "          'type',\n",
    "                     'original_changed_fraction_text_part', 'new_changed_fraction_text_part', \n",
    "              'levenshtein_maximalized', 'nr_insert_max', 'nr_delete_max', 'nr_replace_max',\n",
    "              'levenshtein_minimalized', 'nr_insert_min', 'nr_delete_min', 'nr_replace_min', 'capitalized_equality',\n",
    "                     'jaccard', 'seqratio', 'text_overlap_original', 'text_overlap_new', \n",
    "          'original_lemmatized_minimized_changed_text', 'new_lemmatized_minimized_changed_text', \n",
    "          'stop_words_ratio',    \n",
    "          'ent_original', 'original_token_length', 'adv_orig', 'noun_orig', 'point_orig', 'comma_orig',\n",
    "              'accent_orig', 'haakje_orig', 'doublepoint_orig', 'hyphen_orig', 'threepoints_orig', 'punct_orig', 'x_orig', 'propn_orig',\n",
    "              'pron_orig', 'det_orig', 'sconj_orig', 'space_orig', 'sym_orig', 'num_orig', 'adp_orig',\n",
    "              'intj_orig', 'aux_orig', 'inf_orig', 'pv_verl_ev_orig', 'pv_verl_mv_orig', 'pv_tgw_ev_orig', 'pv_tgw_mv_orig',\n",
    "                'od_prenom_orig', 'od_nom_orig', 'od_postnom_orig', 'od_vrij_orig', 'vd_vrij_orig', 'vd_prenom_orig', \n",
    "              'vd_postnom_orig', 'vd_nom_orig', 'verb_orig', 'cconj_orig', 'adj_sup_orig', 'adj_comp_orig', 'adj_basis_orig',\n",
    "              'ent_new', 'new_token_length', 'adv_new', 'noun_new', 'point_new', 'comma_new',\n",
    "              'accent_new', 'haakje_new', 'doublepoint_new', 'hyphen_new', 'threepoints_new', 'punct_new', 'x_new', 'propn_new',\n",
    "              'pron_new', 'det_new', 'sconj_new', 'space_new', 'sym_new', 'num_new', 'adp_new',\n",
    "              'intj_new', 'aux_new', 'inf_new', 'pv_verl_ev_new', 'pv_verl_mv_new', 'pv_tgw_ev_new', 'pv_tgw_mv_new',\n",
    "                'od_prenom_new', 'od_nom_new', 'od_postnom_new', 'od_vrij_new', 'vd_vrij_new', 'vd_prenom_new', \n",
    "              'vd_postnom_new', 'vd_nom_new', 'verb_new', 'cconj_new', 'adj_sup_new', 'adj_comp_new', 'adj_basis_new',\n",
    "              'double_word', 'equal_after_subst', 'globally_equal_after_subst',\n",
    "              'first_wordtype', 'last_wordtype', 'one_edit_change',\n",
    "               'orginal_spelling_ok', 'new_spelling_ok', 'number_comparison', 'temporary', 'sentence_sim', 'diff_sim',\n",
    "              'doubt_words_orig', 'doubt_words_new', 'doubt_words_total',\n",
    "              'entity_present_in_original', 'entity_present_in_new',\n",
    "              'changed_position',\n",
    "              'colors', 'days', 'currencies', 'months', 'winds', 'states', 'countries', 'cities', 'belgian', 'nationality',\n",
    "              'date_diff', 'person_diff',\n",
    "              'negation_original', 'negation_new',\n",
    "              'nr_red_parts', 'nr_green_parts', 'nr_full_sentences_original', 'nr_full_sentences_new',\n",
    "              'orig_part_of_new', 'new_part_of_orig',\n",
    "                'original_minimized_changed_text', 'new_minimized_changed_text',\n",
    "              'original_changed_text', 'new_changed_text'\n",
    "             ]\n",
    "\n",
    "header_bertje_original = ['original_bertje_' + str(i) for i in range(0, 768)]\n",
    "header_bertje_new = ['new_bertje_' + str(i) for i in range(0, 768)]\n",
    "\n",
    "header_bertje_lemmatized_original = ['original_lemmatized_bertje_' + str(i) for i in range(0, 768)]\n",
    "header_bertje_lemmatized_new = ['new_lemmatized_bertje_' + str(i) for i in range(0, 768)]\n",
    "\n",
    "header_bertje_minimized_original = ['original_minimized_bertje_' + str(i) for i in range(0, 768)]\n",
    "header_bertje_minimized_new = ['new_minimized_bertje_' + str(i) for i in range(0, 768)]\n",
    "\n",
    "header_sbert_original = ['original_sbert_' + str(i) for i in range(0, 512)]\n",
    "header_sbert_new = ['new_sbert_' + str(i) for i in range(0, 512)]\n",
    "\n",
    "header_sbert_lemmatized_original = ['original_lemmatized_sbert_' + str(i) for i in range(0, 512)]\n",
    "header_sbert_lemmatized_new = ['new_lemmatized_sbert_' + str(i) for i in range(0, 512)]\n",
    "\n",
    "header_sbert_minimized_original = ['original_minimized_sbert_' + str(i) for i in range(0, 512)]\n",
    "header_sbert_minimized_new = ['new_minimized_sbert_' + str(i) for i in range(0, 512)]\n",
    "\n",
    "header.extend(header_bertje_original)\n",
    "header.extend(header_bertje_new)\n",
    "header.extend(header_bertje_lemmatized_original)\n",
    "header.extend(header_bertje_lemmatized_new)\n",
    "header.extend(header_bertje_minimized_original)\n",
    "header.extend(header_bertje_minimized_new)\n",
    "        \n",
    "header.extend(header_sbert_original)\n",
    "header.extend(header_sbert_new)\n",
    "header.extend(header_sbert_lemmatized_original)\n",
    "header.extend(header_sbert_lemmatized_new)\n",
    "header.extend(header_sbert_minimized_original)\n",
    "header.extend(header_sbert_minimized_new)\n",
    "        \n",
    "test_writer.writerow(header)\n",
    "\n",
    "train_writer = csv.writer(new_train_file, delimiter=',', quotechar='\"')      \n",
    "train_writer.writerow(header)\n",
    "\n",
    "\n",
    "all_lines = list(reader)[1:]\n",
    "random.shuffle(all_lines)\n",
    "true_lines = [line for line in all_lines if line[32] == 'true']\n",
    "true_lines_indices = [i for i in range(0, len(true_lines))]\n",
    "false_lines = [line for line in all_lines if line[32] == 'false']\n",
    "false_lines_indices = [i for i in range(0, len(false_lines))]\n",
    "\n",
    "true_test_lines_indices = random.sample(true_lines_indices, k=round(0.2*len(true_lines_indices)))\n",
    "true_train_lines_indices = [i for i in true_lines_indices if i not in true_test_lines_indices]\n",
    "true_test_lines = [true_lines[i] for i in true_test_lines_indices]\n",
    "true_train_lines = [true_lines[i] for i in true_train_lines_indices]\n",
    "\n",
    "\n",
    "false_test_lines_indices = random.sample(false_lines_indices, k=round(0.2*len(false_lines_indices)))\n",
    "false_train_lines_indices = [i for i in false_lines_indices if i not in false_test_lines_indices]\n",
    "false_test_lines = [false_lines[i] for i in false_test_lines_indices]\n",
    "false_train_lines = [false_lines[i] for i in false_train_lines_indices]\n",
    "\n",
    "\n",
    "for row in true_test_lines:\n",
    "    test_writer.writerow(row)\n",
    "    \n",
    "for row in false_test_lines:\n",
    "    test_writer.writerow(row)\n",
    "    \n",
    "\n",
    "for row in true_train_lines:\n",
    "    train_writer.writerow(row)\n",
    "    \n",
    "for row in false_train_lines:\n",
    "    train_writer.writerow(row)\n",
    "    \n",
    "new_test_file.close()\n",
    "new_train_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
